{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Autonomous Drones with Aruco Markers \u00b6 Welcome to this tutorial in which we aim to fly both a real and simulated drone around some aruco markers! This tutorial is designed for the 24-25 COMP0240 Aerial Robotics: Fundamentals to Practice in Real-World Environments. Contents \u00b6 In this tutorial, we will be attempting the following: An introduction to drones, the parts of a drone and how to build one. Drone control and the system architectures for drone flight Simulating drone flight and the software architectures used Detecing ARUCO Fiducial markers in real time Autonomously flying a drone around ARUCO markers in simulation Autonomously flying a drone around ARUCO markers in reality Contact \u00b6 This project is developed by the following people: Mickey Li - email Chris Bendowski Vijay Pawar Guohao Wang","title":"Autonomous Drones with Aruco Markers"},{"location":"#autonomous-drones-with-aruco-markers","text":"Welcome to this tutorial in which we aim to fly both a real and simulated drone around some aruco markers! This tutorial is designed for the 24-25 COMP0240 Aerial Robotics: Fundamentals to Practice in Real-World Environments.","title":"Autonomous Drones with Aruco Markers"},{"location":"#contents","text":"In this tutorial, we will be attempting the following: An introduction to drones, the parts of a drone and how to build one. Drone control and the system architectures for drone flight Simulating drone flight and the software architectures used Detecing ARUCO Fiducial markers in real time Autonomously flying a drone around ARUCO markers in simulation Autonomously flying a drone around ARUCO markers in reality","title":"Contents"},{"location":"#contact","text":"This project is developed by the following people: Mickey Li - email Chris Bendowski Vijay Pawar Guohao Wang","title":"Contact"},{"location":"drone/build_qav250_exercise/","text":"QAV250 Build Guide (No Hints) \u00b6 This page contains a no-hints set of links pointing out how to build a QAV250 drone kit to the one we have specced out here at UCL for our drone research QAV250 Build Guide (No Hints) Building a QAV250 Build Instruction Sheet Power Distribution Board and ESCs Motors Pixhawk 6C Flight Controller Connecting the SIK Radio Connecting the remote control (aka receiver Rx and transmitter Tx) Attaching the Raspberry Pi Companion Computer Attaching the rangefinder Drone Setup Ground Station - QGroundControl Building a QAV250 \u00b6 As mentioned in the drone basics chapter, broadly a drone's physical components can be split into four types Chassis Flight Controller Motors and Power Electronics Communications and Sensors You will need to have a very careful think about the order that you build and construct the various parts, the wrong order might cause screws to be covered up, or connectors to be blocked up. Resources: PX4 QAV250 Build Guide (OLD) Build Instruction Sheet \u00b6 The Pdf link of the instruction sheet printed out on the table It is recommended that you don't put the top plate on until you are sure everything is up and ready to fly Power Distribution Board and ESCs \u00b6 One of the first components you will come across is the spidery looking bunch of cables attached to a central board The main board is what is known as the power distribution board and is responsible for powering all the components on the drone, as well as handling the high current draws required by the motors! Each of the four legs contains what is known as the electronic speed controller (ESC). The ESC manages the speed (current draw) of the motor using an input signal from the flight controller. The input signal is often a specific PWM aka pulse width modulation signal. The cables here are rather thick to enable the high current that is required to power the motors. This is a key component in flight. The signal cables plug into the back of the FC in slots 1,2,3 and 4. Note: Watch the orientation of the ESCs when you attach them, they are soldered in such a way that two of them are intentionally upside down when laid flat. Note: You will need to double check the order of the ESC cables are plugged into the FC when you get to running the software configuration. Making a mistake here may cause the drone to be uncontrollable! Rea The final component attached by the thin red and black wires are a video transmitter powering module. This is a second module used to transmit video data in analogue but low latency manner to a receiver. Here is a link to the device, work out how the video transmission is powered! Note: that one of the connectors is shown to connect to the flight controller. We are replacing this connection with a connection from the FC to an onboard computer instead. Motors \u00b6 You will see that there are no instructions on installing the motors! Hopefully you have a good idea of where the motors are meant to go! Try and install them so that the wires go towards the chasis and dont poke out the side. We have printed some propellor guards for the drone, these go under the arm using the same mounting points as the motors. You will need the m3x8mm bolts. Ensure that they are securely fastened. We have also printed off some longer landing legs for the drone too to accomodate the companion computer underneath. Attach those with appropriate bolts and nylock nuts too. DO NOT INSTALL THE PROPELLORS YET The motors should plug into the ESCs in a sraightforward manner with left cable going to left banana plug, center to center and right to right. Compare your setup with the example drone before you use a ziptie to fix down the ESC. Pixhawk 6C Flight Controller \u00b6 The QAV250 uses the Pixhawk 6C Flight controller. This is essentially a small computer (embedded microcontroller) which is designed to support the low level high frequency control of a UAV (e.g. stable flight), communications and basic sensor processing. The 6C has a number of ports on all sides of the drone - read a bit about it on the docs to work out what each one is for! This will have a lot of cables coming out of it, but you probably want to work out how to power it! Note: Use the custom cable with the jumper cables coming out of them - this will be used for powering the companion computer later. Connecting the SIK Radio \u00b6 The second communication method (after the passive video feed communication method) is using the SIK Radios on 433 MHz. There are a pair of them, one to go onboard the drone, and the other is designed to plug into your computer by USB! This should be plugged into TELEM1 of the FC and it should start transmitting data to the reciever when turned on. Connecting the remote control (aka receiver Rx and transmitter Tx) \u00b6 We have bought some flysky i6x Transmitters (Txs) which can fly the drones. These come in two parts, a Tx ad a receiver (RX). In the most basic drone setups, you can directly control all of the motors from a Rx! However we have a slightly more complex setup to enable autonomy. The receiver is the following: Try and work out how it might connect into the flight controller, and also get powered on! Hint: You will need to plug in two of the cables between the Rx and the FC, one of them is a custom cable. We have printed out a custom holder for it - slot the Rx into the holder and use some M2 screws to tighten it down. Antennae thread through the cyliners. Then use some spare double sided sticky tape to attach it to the top plate towards the front (doesn't get in the way of battery). Attaching the Raspberry Pi Companion Computer \u00b6 Now you may have noticed a large component that is not on the original instructions. We want to add autonomy, and that will often involve adding a companion computer to the drone. The purpose of the companion computer is to perform higher level functionality involved with completing the task. This may be mission planning, path planning, computer vision, higher level comms etc. Here we have chosen to use a Raspberry Pi 4B single board computer as its a nice little compact package which has enough compute and compatibility to help perform the autonomy we are aiming for. (In this case run ROS2!) Place the raspberry pi into the 3D printed case and carefully screw it in. Then screw in the raspberry pi camera into the top surface of the case using some m2 nuts and bolts. Use the dual-loc velcro like tape to attach it to the bottom of the drone! Make sure you don't cover up any of the screw access holes! The pi will be both powered and connected to the FC via its GPIO (general purpose IO pins). For powering, the PI will be directly powered using the 5v pin - you can plug the connector into the spot with a 5v next to a ground! For talking to the FC, you can use the custom cable to connect the TELEM2 port to the UART pins of the Pi. See the following diagram for details: NOTE: Make sure you plug the power connector the right way around. Plugging it in the wrong way or in the wrong pins can blow the Pi. Attaching the rangefinder \u00b6 We have also included the VL53L1X rangefinder sensor - this uses a light or infrared beam to work out the distance to an object. In our case we will be mounting it to the bottom of our drone to tell us its height. Use some of the leftover tape to stick it to the underside towards the front, under the video transmitter unit. This will need to plug into the FC, but have a think of where it might go as a sensor! Drone Setup \u00b6 In this section we go over how to setup the drone to prepare it for first flight Ground Station - QGroundControl \u00b6 First things first, you should download QGroundControl . This is a ground station software that PX4 based projects use. It should support all platforms.","title":"QAV250 Build Guide"},{"location":"drone/build_qav250_exercise/#qav250-build-guide-no-hints","text":"This page contains a no-hints set of links pointing out how to build a QAV250 drone kit to the one we have specced out here at UCL for our drone research QAV250 Build Guide (No Hints) Building a QAV250 Build Instruction Sheet Power Distribution Board and ESCs Motors Pixhawk 6C Flight Controller Connecting the SIK Radio Connecting the remote control (aka receiver Rx and transmitter Tx) Attaching the Raspberry Pi Companion Computer Attaching the rangefinder Drone Setup Ground Station - QGroundControl","title":"QAV250 Build Guide (No Hints)"},{"location":"drone/build_qav250_exercise/#building-a-qav250","text":"As mentioned in the drone basics chapter, broadly a drone's physical components can be split into four types Chassis Flight Controller Motors and Power Electronics Communications and Sensors You will need to have a very careful think about the order that you build and construct the various parts, the wrong order might cause screws to be covered up, or connectors to be blocked up. Resources: PX4 QAV250 Build Guide (OLD)","title":"Building a QAV250"},{"location":"drone/build_qav250_exercise/#build-instruction-sheet","text":"The Pdf link of the instruction sheet printed out on the table It is recommended that you don't put the top plate on until you are sure everything is up and ready to fly","title":"Build Instruction Sheet"},{"location":"drone/build_qav250_exercise/#power-distribution-board-and-escs","text":"One of the first components you will come across is the spidery looking bunch of cables attached to a central board The main board is what is known as the power distribution board and is responsible for powering all the components on the drone, as well as handling the high current draws required by the motors! Each of the four legs contains what is known as the electronic speed controller (ESC). The ESC manages the speed (current draw) of the motor using an input signal from the flight controller. The input signal is often a specific PWM aka pulse width modulation signal. The cables here are rather thick to enable the high current that is required to power the motors. This is a key component in flight. The signal cables plug into the back of the FC in slots 1,2,3 and 4. Note: Watch the orientation of the ESCs when you attach them, they are soldered in such a way that two of them are intentionally upside down when laid flat. Note: You will need to double check the order of the ESC cables are plugged into the FC when you get to running the software configuration. Making a mistake here may cause the drone to be uncontrollable! Rea The final component attached by the thin red and black wires are a video transmitter powering module. This is a second module used to transmit video data in analogue but low latency manner to a receiver. Here is a link to the device, work out how the video transmission is powered! Note: that one of the connectors is shown to connect to the flight controller. We are replacing this connection with a connection from the FC to an onboard computer instead.","title":"Power Distribution Board and ESCs"},{"location":"drone/build_qav250_exercise/#motors","text":"You will see that there are no instructions on installing the motors! Hopefully you have a good idea of where the motors are meant to go! Try and install them so that the wires go towards the chasis and dont poke out the side. We have printed some propellor guards for the drone, these go under the arm using the same mounting points as the motors. You will need the m3x8mm bolts. Ensure that they are securely fastened. We have also printed off some longer landing legs for the drone too to accomodate the companion computer underneath. Attach those with appropriate bolts and nylock nuts too. DO NOT INSTALL THE PROPELLORS YET The motors should plug into the ESCs in a sraightforward manner with left cable going to left banana plug, center to center and right to right. Compare your setup with the example drone before you use a ziptie to fix down the ESC.","title":"Motors"},{"location":"drone/build_qav250_exercise/#pixhawk-6c-flight-controller","text":"The QAV250 uses the Pixhawk 6C Flight controller. This is essentially a small computer (embedded microcontroller) which is designed to support the low level high frequency control of a UAV (e.g. stable flight), communications and basic sensor processing. The 6C has a number of ports on all sides of the drone - read a bit about it on the docs to work out what each one is for! This will have a lot of cables coming out of it, but you probably want to work out how to power it! Note: Use the custom cable with the jumper cables coming out of them - this will be used for powering the companion computer later.","title":"Pixhawk 6C Flight Controller"},{"location":"drone/build_qav250_exercise/#connecting-the-sik-radio","text":"The second communication method (after the passive video feed communication method) is using the SIK Radios on 433 MHz. There are a pair of them, one to go onboard the drone, and the other is designed to plug into your computer by USB! This should be plugged into TELEM1 of the FC and it should start transmitting data to the reciever when turned on.","title":"Connecting the SIK Radio"},{"location":"drone/build_qav250_exercise/#connecting-the-remote-control-aka-receiver-rx-and-transmitter-tx","text":"We have bought some flysky i6x Transmitters (Txs) which can fly the drones. These come in two parts, a Tx ad a receiver (RX). In the most basic drone setups, you can directly control all of the motors from a Rx! However we have a slightly more complex setup to enable autonomy. The receiver is the following: Try and work out how it might connect into the flight controller, and also get powered on! Hint: You will need to plug in two of the cables between the Rx and the FC, one of them is a custom cable. We have printed out a custom holder for it - slot the Rx into the holder and use some M2 screws to tighten it down. Antennae thread through the cyliners. Then use some spare double sided sticky tape to attach it to the top plate towards the front (doesn't get in the way of battery).","title":"Connecting the remote control (aka receiver Rx and transmitter Tx)"},{"location":"drone/build_qav250_exercise/#attaching-the-raspberry-pi-companion-computer","text":"Now you may have noticed a large component that is not on the original instructions. We want to add autonomy, and that will often involve adding a companion computer to the drone. The purpose of the companion computer is to perform higher level functionality involved with completing the task. This may be mission planning, path planning, computer vision, higher level comms etc. Here we have chosen to use a Raspberry Pi 4B single board computer as its a nice little compact package which has enough compute and compatibility to help perform the autonomy we are aiming for. (In this case run ROS2!) Place the raspberry pi into the 3D printed case and carefully screw it in. Then screw in the raspberry pi camera into the top surface of the case using some m2 nuts and bolts. Use the dual-loc velcro like tape to attach it to the bottom of the drone! Make sure you don't cover up any of the screw access holes! The pi will be both powered and connected to the FC via its GPIO (general purpose IO pins). For powering, the PI will be directly powered using the 5v pin - you can plug the connector into the spot with a 5v next to a ground! For talking to the FC, you can use the custom cable to connect the TELEM2 port to the UART pins of the Pi. See the following diagram for details: NOTE: Make sure you plug the power connector the right way around. Plugging it in the wrong way or in the wrong pins can blow the Pi.","title":"Attaching the Raspberry Pi Companion Computer"},{"location":"drone/build_qav250_exercise/#attaching-the-rangefinder","text":"We have also included the VL53L1X rangefinder sensor - this uses a light or infrared beam to work out the distance to an object. In our case we will be mounting it to the bottom of our drone to tell us its height. Use some of the leftover tape to stick it to the underside towards the front, under the video transmitter unit. This will need to plug into the FC, but have a think of where it might go as a sensor!","title":"Attaching the rangefinder"},{"location":"drone/build_qav250_exercise/#drone-setup","text":"In this section we go over how to setup the drone to prepare it for first flight","title":"Drone Setup"},{"location":"drone/build_qav250_exercise/#ground-station-qgroundcontrol","text":"First things first, you should download QGroundControl . This is a ground station software that PX4 based projects use. It should support all platforms.","title":"Ground Station - QGroundControl"},{"location":"drone/drone_basics/","text":"Introduction to Drone Systems \u00b6 This tutorial page gives you a brief overview of a drone system and the physical and software components that make up a drone system Introduction to Drone Systems What is a UAV or a Drone Types of Drones Piloting Drones Parts of a drone What is a UAV or a Drone \u00b6 A drone or unmanned aerial vehicle (UAV) is an unmanned \"robotic\" vehicle that can be remotely or autonomously controlled. Drones are used for many consumer, industrial, government and military applications (opens new window). These include (non exhaustively): aerial photography/video, carrying cargo, racing, search and surveying etc. Different types of drones exist for use in air, ground, sea, and underwater. These are (more formally) referred to as Unmanned Aerial Vehicles (UAV), Unmanned Aerial Systems (UAS), Unmanned Ground Vehicles (UGV), Unmanned Surface Vehicles (USV), Unmanned Underwater Vehicles (UUV). The \"brain\" of the drone is called an autopilot . It consists of flight stack software running on vehicle controller (\"flight controller\") hardware. A multi-rotor is a specific type of UAV which uses two of more lift-generating rotors to fly. One of the most common will be the Quadrotor which has 4 motors in an 'X' pattern. These UAVs provide much simpler flight control than other types of aerial vehicle. This tutorial focuses on the flight of a simple quadrotor, but Starling can be used to operate many different types of robot. From this point on in this tutorial, 'drone' or 'UAV' will refer to a multi-rotor UAV unless otherwise stated. Types of Drones \u00b6 These drones are used for many purposes, most notably in photography and education. Another type are fixed-wing drones, which resemble miniature airplanes and excel in long-range missions and mapping tasks. A third type are hybrid drones which combine the features of multirotor and fixed-wing drones, offering more versatility and extended flight time. Each drone type has its advantages and is suited to specific applications and operational requirements. Piloting Drones \u00b6 Piloting drones takes a bit of practice, but it\u2019s accessible enough that even kids can do it! The controller or joystick helps you to navigate your drone where you want it to go. Once airborne, there are four basic movements of drone flight that need to be mastered: Throttle \u2013 vertical movement (up and down), sometimes called altitude. Yaw \u2013 rotation from left to right Pitch \u2013 backward and forward movement Roll \u2013 left to right movement without rotation Once you get used to the terminology and the orientation of the drone, the sky is the literal limit! Note that in more complicated systems such as the one we use today there are other modes to! The Yaw, Pitch, Roll above describes the attitude of the vehicle. Acrobatic and FPV drones usually operate on attitude rates, where the controller gives Pitch rates or yaw rates. This is desirable as it gives the feeling of immediate reaction to controls - although that does mean you have to actively stabilise the drone yourself. On the other side, even higher level control methods exist which translte attitude to cartesian x,y,z frames. Often drones end up being either velocity controlled or position controlled. The benefits of these higher level control is that you can command them to go to places relative to other things. The only downside is that the drone needs to know where it is in relation to the world - either using GPS, indoor positioning or onboard SLAM (simultaneous localisation and mapping) solutions. Parts of a drone \u00b6","title":"Introduction to Drones"},{"location":"drone/drone_basics/#introduction-to-drone-systems","text":"This tutorial page gives you a brief overview of a drone system and the physical and software components that make up a drone system Introduction to Drone Systems What is a UAV or a Drone Types of Drones Piloting Drones Parts of a drone","title":"Introduction to Drone Systems"},{"location":"drone/drone_basics/#what-is-a-uav-or-a-drone","text":"A drone or unmanned aerial vehicle (UAV) is an unmanned \"robotic\" vehicle that can be remotely or autonomously controlled. Drones are used for many consumer, industrial, government and military applications (opens new window). These include (non exhaustively): aerial photography/video, carrying cargo, racing, search and surveying etc. Different types of drones exist for use in air, ground, sea, and underwater. These are (more formally) referred to as Unmanned Aerial Vehicles (UAV), Unmanned Aerial Systems (UAS), Unmanned Ground Vehicles (UGV), Unmanned Surface Vehicles (USV), Unmanned Underwater Vehicles (UUV). The \"brain\" of the drone is called an autopilot . It consists of flight stack software running on vehicle controller (\"flight controller\") hardware. A multi-rotor is a specific type of UAV which uses two of more lift-generating rotors to fly. One of the most common will be the Quadrotor which has 4 motors in an 'X' pattern. These UAVs provide much simpler flight control than other types of aerial vehicle. This tutorial focuses on the flight of a simple quadrotor, but Starling can be used to operate many different types of robot. From this point on in this tutorial, 'drone' or 'UAV' will refer to a multi-rotor UAV unless otherwise stated.","title":"What is a UAV or a Drone"},{"location":"drone/drone_basics/#types-of-drones","text":"These drones are used for many purposes, most notably in photography and education. Another type are fixed-wing drones, which resemble miniature airplanes and excel in long-range missions and mapping tasks. A third type are hybrid drones which combine the features of multirotor and fixed-wing drones, offering more versatility and extended flight time. Each drone type has its advantages and is suited to specific applications and operational requirements.","title":"Types of Drones"},{"location":"drone/drone_basics/#piloting-drones","text":"Piloting drones takes a bit of practice, but it\u2019s accessible enough that even kids can do it! The controller or joystick helps you to navigate your drone where you want it to go. Once airborne, there are four basic movements of drone flight that need to be mastered: Throttle \u2013 vertical movement (up and down), sometimes called altitude. Yaw \u2013 rotation from left to right Pitch \u2013 backward and forward movement Roll \u2013 left to right movement without rotation Once you get used to the terminology and the orientation of the drone, the sky is the literal limit! Note that in more complicated systems such as the one we use today there are other modes to! The Yaw, Pitch, Roll above describes the attitude of the vehicle. Acrobatic and FPV drones usually operate on attitude rates, where the controller gives Pitch rates or yaw rates. This is desirable as it gives the feeling of immediate reaction to controls - although that does mean you have to actively stabilise the drone yourself. On the other side, even higher level control methods exist which translte attitude to cartesian x,y,z frames. Often drones end up being either velocity controlled or position controlled. The benefits of these higher level control is that you can command them to go to places relative to other things. The only downside is that the drone needs to know where it is in relation to the world - either using GPS, indoor positioning or onboard SLAM (simultaneous localisation and mapping) solutions.","title":"Piloting Drones"},{"location":"drone/drone_basics/#parts-of-a-drone","text":"","title":"Parts of a drone"},{"location":"simulation/example/","text":"Example Mission \u00b6 Following on from the installation, let us talk about running the example mission. Example Mission Running the simulator What does the simulator run Controlling with teleoperation Running the example autonomous mission Stopping the simulator Disecting the controller Modifying the controller Transferring this controller Running the simulator \u00b6 In your selected environment, start the aerostack2 simulation environment cd /ros2/project_gazebo_aruco ./launch_as2.bash -s -t -v The launch_as2.bash file runs a bash script which controls what elements are also started up. The -s indicates that the system should be launched in simulation mode. The -t opens up the teleoperation remote control window to control the drone. The -v opens up a visualisation software called rviz2. What does the simulator run \u00b6 The aerostack simulator runs a number of different modules for ensuring successful flight this includes: Platform Interface State Estimation Motion Controllers Behaviour Controllers (Takeoff, Land, Go-To) You can see the status of these other modules if you select the terminal and press Ctrl + B and then press 0 ... 5 . This terminal environment is called tmux Controlling with teleoperation \u00b6 Using the -t option will open up the teleoperation panel, allowing you to attempt to manually fly the robot in simulation. With the teleoperation panel clicked on and selected, you can press the T key to takeoff. The arrow keys will then control the direction of flight of the drone in position control mode (you tell the drone where specifically to go). Notice how the drone flies around in the simulation and in rviz. Also see how the image within rviz changes! The image is from a bottom mounted camera on the simulated drone, mirroring the setup you will be building with the real drone. Play around a fly the drone around! Running the example autonomous mission \u00b6 In the original terminal (Use Ctrl + B + 5 ), we can run any scripts we want. The mission_*.py scripts are some examples of these autonomous missions. To run one of these examples, in the terminal type: python3 mission_camera.py This example mission gives you a camera stream from the drone, takes off and arms the drone and flies it around using a few different methods. Key points is the use of ROS2 to subscribe to the camera topic published by the drone model. Whenever an image is received, it will run the img_callback function. Stopping the simulator \u00b6 In order to stop the simulator cleanly, in any terminal run the ./stop.bash script. ./stop.bash This will stop all containers and relevant programs to the simulator in a clean manner. Sometimes the simulated drone will go into an unrecoverable state - you may need to resart the simulator. Disecting the controller \u00b6 Here is the example controller. It is documented! #!/bin/python3 \"\"\" CAMERA SAMPLE MISSION This file is an example mission which reads from the aerostack drone camera and prints it to screen It also flies around using position and velocity control camera topic \"\"\" # Imports import time import rclpy import argparse from as2_python_api.drone_interface import DroneInterface from rclpy.qos import qos_profile_sensor_data from sensor_msgs.msg import Image, CameraInfo from cv_bridge import CvBridge import cv2 ######## Drone Control Class ################### class DroneMotionRef(DroneInterface): \"\"\"Drone Interface This is the aerostack2 drone interface for connecting to simulated and real drones. It runs as a ROS2 Node which interacts with the currently available ROS2 topics. It defines the variables that represent a single drone, i.e. - Platform Information - Vehicle Pose and Twist (angular velocity) - Functions to control the hardware of the drone (arm, disarm, change mode, estop) It also contains some modules for flying the drone, this includes: - Takeoff, Landing (self.takeoff, self.land) - GoTo position control (self.go_to) [https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/modules/go_to_module.py] - FollowPath module (self.follow_path) [https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/modules/follow_path_module.py] Other module exist which could be used to. Their interfaces and functions can be referenced most easily in the code. Some Documentation is here: https://aerostack2.github.io/_09_development/_api_documentation/temp_ws/src/as2_python_api/docs/source/as2_python_api.html The Source Code is here: https://github.com/aerostack2/aerostack2/tree/main/as2_python_api Drone Interface Base.py: https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/drone_interface_base.py Drone Interface.py: https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/drone_interface.py \"\"\" def __init__(self, name, verbose=False, use_sim_time=False): super().__init__(name, verbose, use_sim_time) # ROS2 create a subscription to the raw image of the sensors. # This details the ros message type (Image), the name of the topic # And the function that should be called when a message is received on this topic self.create_subscription(Image, \"sensor_measurements/hd_camera/image_raw\", self.img_callback, qos_profile_sensor_data) # CV Bridge is a set of functions to convert to and from ROS images to Opencv images self.br = CvBridge() def img_callback(self, data): \"\"\"Image Callback Function The image message is defined here: https://github.com/ros2/common_interfaces/blob/rolling/sensor_msgs/msg/Image.msg Args: data (sensor_msgs.msg.Image): The received image message \"\"\" self.get_logger().info('Receiving video frame', once=True) # Log Once # Convert the image message to a Opencv image frame current_frame = self.br.imgmsg_to_cv2(data) # Show the frame in a window cv2.imshow(\"camera\", current_frame) cv2.waitKey(1) # Wait a millisecond def run_test(self): \"\"\" Run the mission \"\"\" # Set the drone to offboard mode. This prepares the drone to receive # commands from outside of the flight controller. self.offboard() self.get_logger().info(\"Offboard Mode\") # Arming the drone powers up the motors to prepare for flight self.arm() self.get_logger().info(\"Armed!\") # Takeoff to 1 meter self.get_logger().info(\"Taking Off!\") res = self.takeoff(height=1.0, speed=0.5) if res: self.get_logger().info(\"Take off complete\") else: self.get_logger().info(\"Take off Failed, exiting\") return # Wait a little bit time.sleep(1.0) # Position Control fly around a bit speed = 1.5 self.go_to.go_to_point([1, 0, 1.0], speed=speed) self.get_logger().info(\"Point 1\") self.go_to.go_to_point([2, 0, 2.0], speed=speed) self.get_logger().info(\"Point 2\") self.go_to.go_to_point([3, 0, 3.0], speed=speed) self.get_logger().info(\"Point 3\") self.go_to.go_to(3.0, -1.0, 2.5, speed=speed) self.get_logger().info(\"Point 4\") self.go_to.go_to_point_with_yaw([4, 1, 3.0], angle=45.0, speed=speed) self.get_logger().info(\"Point 5\") self.go_to.go_to_point_with_yaw([3, -2, 2.0], angle=-45.0, speed=speed) self.get_logger().info(\"Point 6\") self.go_to.go_to_point_with_yaw([0, 0, 1.0], angle=0.0, speed=speed) self.get_logger().info(\"Point 7\") self.land() ############# Running the mission and Entrypoint ################################# if __name__ == '__main__': parser = argparse.ArgumentParser( description=\"Starts camera mission\") parser.add_argument('-s', '--simulated', action='store_true', default=False) parser.add_argument('-n', '--drone_name', default=\"cf0\") args = parser.parse_args() if args.simulated: print(\"Mission running in simulation mode\") else: print(\"Mission running in real mode\") # Starts ROS2 Node in a script rclpy.init() # Create the drone object. Connects to the real/simulated drone and runs tests uav = DroneMotionRef(args.drone_name, verbose=True) # Runs the UAV TEST function uav.run_test() # Shuts down the UAV uav.shutdown() # Stop ROS2 Node rclpy.shutdown() print(\"Clean exit\") exit(0) Modifying the controller \u00b6 Now try and modify this controller by making it go to different places at different speeds. This would be a good time to try an automate any computer vision or detection algorithms inside this python script. Transferring this controller \u00b6 To transfer this controller to real drone, all that is needed is to copy this python mission file to the running machine.","title":"Example Scenario"},{"location":"simulation/example/#example-mission","text":"Following on from the installation, let us talk about running the example mission. Example Mission Running the simulator What does the simulator run Controlling with teleoperation Running the example autonomous mission Stopping the simulator Disecting the controller Modifying the controller Transferring this controller","title":"Example Mission"},{"location":"simulation/example/#running-the-simulator","text":"In your selected environment, start the aerostack2 simulation environment cd /ros2/project_gazebo_aruco ./launch_as2.bash -s -t -v The launch_as2.bash file runs a bash script which controls what elements are also started up. The -s indicates that the system should be launched in simulation mode. The -t opens up the teleoperation remote control window to control the drone. The -v opens up a visualisation software called rviz2.","title":"Running the simulator"},{"location":"simulation/example/#what-does-the-simulator-run","text":"The aerostack simulator runs a number of different modules for ensuring successful flight this includes: Platform Interface State Estimation Motion Controllers Behaviour Controllers (Takeoff, Land, Go-To) You can see the status of these other modules if you select the terminal and press Ctrl + B and then press 0 ... 5 . This terminal environment is called tmux","title":"What does the simulator run"},{"location":"simulation/example/#controlling-with-teleoperation","text":"Using the -t option will open up the teleoperation panel, allowing you to attempt to manually fly the robot in simulation. With the teleoperation panel clicked on and selected, you can press the T key to takeoff. The arrow keys will then control the direction of flight of the drone in position control mode (you tell the drone where specifically to go). Notice how the drone flies around in the simulation and in rviz. Also see how the image within rviz changes! The image is from a bottom mounted camera on the simulated drone, mirroring the setup you will be building with the real drone. Play around a fly the drone around!","title":"Controlling with teleoperation"},{"location":"simulation/example/#running-the-example-autonomous-mission","text":"In the original terminal (Use Ctrl + B + 5 ), we can run any scripts we want. The mission_*.py scripts are some examples of these autonomous missions. To run one of these examples, in the terminal type: python3 mission_camera.py This example mission gives you a camera stream from the drone, takes off and arms the drone and flies it around using a few different methods. Key points is the use of ROS2 to subscribe to the camera topic published by the drone model. Whenever an image is received, it will run the img_callback function.","title":"Running the example autonomous mission"},{"location":"simulation/example/#stopping-the-simulator","text":"In order to stop the simulator cleanly, in any terminal run the ./stop.bash script. ./stop.bash This will stop all containers and relevant programs to the simulator in a clean manner. Sometimes the simulated drone will go into an unrecoverable state - you may need to resart the simulator.","title":"Stopping the simulator"},{"location":"simulation/example/#disecting-the-controller","text":"Here is the example controller. It is documented! #!/bin/python3 \"\"\" CAMERA SAMPLE MISSION This file is an example mission which reads from the aerostack drone camera and prints it to screen It also flies around using position and velocity control camera topic \"\"\" # Imports import time import rclpy import argparse from as2_python_api.drone_interface import DroneInterface from rclpy.qos import qos_profile_sensor_data from sensor_msgs.msg import Image, CameraInfo from cv_bridge import CvBridge import cv2 ######## Drone Control Class ################### class DroneMotionRef(DroneInterface): \"\"\"Drone Interface This is the aerostack2 drone interface for connecting to simulated and real drones. It runs as a ROS2 Node which interacts with the currently available ROS2 topics. It defines the variables that represent a single drone, i.e. - Platform Information - Vehicle Pose and Twist (angular velocity) - Functions to control the hardware of the drone (arm, disarm, change mode, estop) It also contains some modules for flying the drone, this includes: - Takeoff, Landing (self.takeoff, self.land) - GoTo position control (self.go_to) [https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/modules/go_to_module.py] - FollowPath module (self.follow_path) [https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/modules/follow_path_module.py] Other module exist which could be used to. Their interfaces and functions can be referenced most easily in the code. Some Documentation is here: https://aerostack2.github.io/_09_development/_api_documentation/temp_ws/src/as2_python_api/docs/source/as2_python_api.html The Source Code is here: https://github.com/aerostack2/aerostack2/tree/main/as2_python_api Drone Interface Base.py: https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/drone_interface_base.py Drone Interface.py: https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/drone_interface.py \"\"\" def __init__(self, name, verbose=False, use_sim_time=False): super().__init__(name, verbose, use_sim_time) # ROS2 create a subscription to the raw image of the sensors. # This details the ros message type (Image), the name of the topic # And the function that should be called when a message is received on this topic self.create_subscription(Image, \"sensor_measurements/hd_camera/image_raw\", self.img_callback, qos_profile_sensor_data) # CV Bridge is a set of functions to convert to and from ROS images to Opencv images self.br = CvBridge() def img_callback(self, data): \"\"\"Image Callback Function The image message is defined here: https://github.com/ros2/common_interfaces/blob/rolling/sensor_msgs/msg/Image.msg Args: data (sensor_msgs.msg.Image): The received image message \"\"\" self.get_logger().info('Receiving video frame', once=True) # Log Once # Convert the image message to a Opencv image frame current_frame = self.br.imgmsg_to_cv2(data) # Show the frame in a window cv2.imshow(\"camera\", current_frame) cv2.waitKey(1) # Wait a millisecond def run_test(self): \"\"\" Run the mission \"\"\" # Set the drone to offboard mode. This prepares the drone to receive # commands from outside of the flight controller. self.offboard() self.get_logger().info(\"Offboard Mode\") # Arming the drone powers up the motors to prepare for flight self.arm() self.get_logger().info(\"Armed!\") # Takeoff to 1 meter self.get_logger().info(\"Taking Off!\") res = self.takeoff(height=1.0, speed=0.5) if res: self.get_logger().info(\"Take off complete\") else: self.get_logger().info(\"Take off Failed, exiting\") return # Wait a little bit time.sleep(1.0) # Position Control fly around a bit speed = 1.5 self.go_to.go_to_point([1, 0, 1.0], speed=speed) self.get_logger().info(\"Point 1\") self.go_to.go_to_point([2, 0, 2.0], speed=speed) self.get_logger().info(\"Point 2\") self.go_to.go_to_point([3, 0, 3.0], speed=speed) self.get_logger().info(\"Point 3\") self.go_to.go_to(3.0, -1.0, 2.5, speed=speed) self.get_logger().info(\"Point 4\") self.go_to.go_to_point_with_yaw([4, 1, 3.0], angle=45.0, speed=speed) self.get_logger().info(\"Point 5\") self.go_to.go_to_point_with_yaw([3, -2, 2.0], angle=-45.0, speed=speed) self.get_logger().info(\"Point 6\") self.go_to.go_to_point_with_yaw([0, 0, 1.0], angle=0.0, speed=speed) self.get_logger().info(\"Point 7\") self.land() ############# Running the mission and Entrypoint ################################# if __name__ == '__main__': parser = argparse.ArgumentParser( description=\"Starts camera mission\") parser.add_argument('-s', '--simulated', action='store_true', default=False) parser.add_argument('-n', '--drone_name', default=\"cf0\") args = parser.parse_args() if args.simulated: print(\"Mission running in simulation mode\") else: print(\"Mission running in real mode\") # Starts ROS2 Node in a script rclpy.init() # Create the drone object. Connects to the real/simulated drone and runs tests uav = DroneMotionRef(args.drone_name, verbose=True) # Runs the UAV TEST function uav.run_test() # Shuts down the UAV uav.shutdown() # Stop ROS2 Node rclpy.shutdown() print(\"Clean exit\") exit(0)","title":"Disecting the controller"},{"location":"simulation/example/#modifying-the-controller","text":"Now try and modify this controller by making it go to different places at different speeds. This would be a good time to try an automate any computer vision or detection algorithms inside this python script.","title":"Modifying the controller"},{"location":"simulation/example/#transferring-this-controller","text":"To transfer this controller to real drone, all that is needed is to copy this python mission file to the running machine.","title":"Transferring this controller"},{"location":"simulation/installation/","text":"Installation the ARUCO Project \u00b6 This page goes over how you run this example aruco project Installation the ARUCO Project Installation Local Setup and build Aerostack2 (we use version 1.1.2) Setup this project Docker Docker with VNC Environment Installation Installing Ubuntu 22.04 Installing ROS2 Humble Installing Ignition Gazebo Fortress Installation \u00b6 There are three ways you can run this project depending on your situation or operating system. Linux: All 3 ways will work Windows: Docker and Docker with VNC Max OSX: Docker with VNC Note that you will need at least 20Gb free on your system to avoid lockup. Graphics card and 16Gb RAM is recommended. Windows You will need to install WSL2 for Docker and these docker containers to work. Instructions Here See Documentation for detailed instructions Local \u00b6 Install Ubuntu 22.04, ROS2 Humble, Ignition Gazebo Fortress as per their instructions. See these instructions for installation Setup and build Aerostack2 (we use version 1.1.2) \u00b6 In your home directory (could be anywhere else but all the paths below are for your home directory) mkdir -p ~/aerostack2_ws/src cd ~/aerostack2_ws/src git clone https://github.com/mhl787156/aerostack2.git -b 1.1.2 This will create a ros2 workspace and place the two project repositories in it. You will need to install the dependencies by running the following: sudo apt install git python3-rosdep python3-pip python3-colcon-common-extensions tmux tmuxinator -y And then going back into the root workspace cd ~/aerostack2_ws sudo rosdep init rosdep update rosdep install -y -r -q --from-paths src --ignore-src Then, enable the handy aerostack2 cli (only run this once) echo 'export AEROSTACK2_PATH=$HOME/aerostack2_ws/src/aerostack2' >> $HOME/.bashrc echo 'source $AEROSTACK2_PATH/as2_cli/setup_env.bash' >> $HOME/.bashrc echo 'source $HOME/aerostack2_ws/install/setup.bash' >> $HOME/.bashrc source ~/.bashrc This will enable you to build the project from any folder using as2 build Now as2 should be installed. Setup this project \u00b6 Get this project locally mkdir -p ~/aerostack2_ws/src cd ~/aerostack2_ws/src git clone https://github.com/ucl-delta/project_gazebo_aruco.git Run the example using cd /ros2/project_gazebo_aruco ./launch_as2.bash -s -t Docker \u00b6 Ensure Docker or Docker Desktop is installed on your machine First your will need to clone this project somewhere (doesn't need to be in a ros2 workspace) git clone https://github.com/ucl-delta/project_gazebo_aruco.git To build and/or run the container run the script This container is based on Ubuntu 22.04, ROS2 Humble and Ignition Gazebo Fortress ./docker/docker_start.bash After building for a while, this will drop you inside the docker container. The container will have live mounted this project into /ros2/project_gazebo_aruco so that any changes made to this repository outside of the container will be reflected inside. Inside the container, navigate to that repository and run the example. cd /ros2/project_gazebo_aruco ./launch_as2.bash -s -t Note that you can utilise a GPU if you install the nvidia-container-toolkit . Pass the -nvidia argument to docker_start.bash Docker with VNC \u00b6 This will enable all of the ROS2 to run standalone with no outside network connections. It makes use of a Virtual Network Computing interface to share a the container's desktop GUI with the outside world. In this case your browser! Ensure Docker or Docker Desktop is installed on your machine First your will need to clone this project somewhere (doesn't need to be in a ros2 workspace) git clone https://github.com/ucl-delta/project_gazebo_aruco.git To build and/or run the container run the script This container is based on Ubuntu 22.04, ROS2 Humble and Ignition Gazebo Fortress ./docker/docker_vnc_start.bash After building for a while, this will say that it has started the VNC server. Note in some instances, the first time you build it it may time out. Run it again and it should finish its build. Go into a browser and navigate to https://127.0.0.1:6080 and press connect . This will drop you into an ubunut22.04 desktop environment with all the things you need! The container will have live mounted this project into /ros2/project_gazebo_aruco so that any changes made to this repository outside of the container will be reflected inside. Open up a terminal ( terminator ) and navigate to /ros2/project_gazebo_aruco to run the example. cd /ros2/project_gazebo_aruco ./launch_as2.bash -s -t Note that you can utilise a GPU if you install the nvidia-container-toolkit . Pass the -nvidia argument to docker_vnc_start.bash when running the vnc. Environment Installation \u00b6 This section applies if you are trying to setup from scratch This project relies on ROS2 Humble (End-of-life 2027), Ignition Gazebo Fortress, and Ubuntu 22.04. This section goes over how to install these elements. You may want to refer to the Intro To Linux Tutorial to setup some of the dependencies. Installing Ubuntu 22.04 \u00b6 Your options are Finding a spare machine that you can wipe and install Ubuntu on Finding a machine that you would be willing to dual boot Running a virtual machine Running a container (Not supported yet as I havent made a container - also this requires gazebo which requires a GUI which is not ideal for containers apart from singularity containers) For options 1 and 2, you will need to find a USB stick that is >4Gb and using a tool such as Rufus flash the Ubuntu 22.04 ISO file onto it. Once you have a flashed USB drive, you can insert that into the spare machine. On startup make sure to mash some combination of F2, F8 or F12 to go to the BIOS boot screen and select boot from USB. This will start up tp the Ubuntu installer on the USB drive where you can select what to do. Whether that is to wipe the machine, or in the advanced menu create a new partition for Ubuntu so you can dual boot (For Windows you will also need to shrink your primary partition). For more details see a guide such as this one . You can also download the ISO file and run it in a virtual machine program such as VirtualBox . I have created and exported a virtual machine with everything already installed for you to use. See the private teams group. In virtualbox you can import an existing virtual machine. Once installed and you have the gui up, there is an option to import (orange arrow), in which you can select the existing exported virtual machine. Once you have a virtual machine setup, you can simply start it. Note: Since this project uses gazebo and is not the lightest workioad, you may want to give the VM more resources i.e. CPU, RAM and potetially video memory too. You can do this in the virtual machine settings. Installing ROS2 Humble \u00b6 For this project we will be using ROS2 Humble. Full installation instructions are here . But in short: locale # check for UTF-8 sudo apt update && sudo apt install locales sudo locale-gen en_US en_US.UTF-8 sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8 export LANG=en_US.UTF-8 locale # verify settings # Install sudo apt install software-properties-common sudo add-apt-repository universe sudo apt update && sudo apt install curl -y sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null # Update sudo apt update sudo apt upgrade # Install ROS2 sudo apt install ros-humble-desktop sudo apt install ros-dev-tools # Auto Source in bashrc to have access to ros2 tools echo 'source ~/opt/ros/huble/setup.bash' >> $HOME/.bashrc Installing Ignition Gazebo Fortress \u00b6 The recommended compatible gazebo version for Ubuntu 22.04 and Humble is Fortress where installation instructions are here . But in short: sudo apt-get update sudo apt-get install lsb-release wget gnupg sudo wget https://packages.osrfoundation.org/gazebo.gpg -O /usr/share/keyrings/pkgs-osrf-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/pkgs-osrf-archive-keyring.gpg] http://packages.osrfoundation.org/gazebo/ubuntu-stable $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/gazebo-stable.list > /dev/null sudo apt-get update sudo apt-get install ignition-fortress You can check succesful installation by using the ign cli command or gz cli command With these three installed, you should be ready to run the sample gazebo aruco simulation","title":"Project Installation"},{"location":"simulation/installation/#installation-the-aruco-project","text":"This page goes over how you run this example aruco project Installation the ARUCO Project Installation Local Setup and build Aerostack2 (we use version 1.1.2) Setup this project Docker Docker with VNC Environment Installation Installing Ubuntu 22.04 Installing ROS2 Humble Installing Ignition Gazebo Fortress","title":"Installation the ARUCO Project"},{"location":"simulation/installation/#installation","text":"There are three ways you can run this project depending on your situation or operating system. Linux: All 3 ways will work Windows: Docker and Docker with VNC Max OSX: Docker with VNC Note that you will need at least 20Gb free on your system to avoid lockup. Graphics card and 16Gb RAM is recommended. Windows You will need to install WSL2 for Docker and these docker containers to work. Instructions Here See Documentation for detailed instructions","title":"Installation"},{"location":"simulation/installation/#local","text":"Install Ubuntu 22.04, ROS2 Humble, Ignition Gazebo Fortress as per their instructions. See these instructions for installation","title":"Local"},{"location":"simulation/installation/#setup-and-build-aerostack2-we-use-version-112","text":"In your home directory (could be anywhere else but all the paths below are for your home directory) mkdir -p ~/aerostack2_ws/src cd ~/aerostack2_ws/src git clone https://github.com/mhl787156/aerostack2.git -b 1.1.2 This will create a ros2 workspace and place the two project repositories in it. You will need to install the dependencies by running the following: sudo apt install git python3-rosdep python3-pip python3-colcon-common-extensions tmux tmuxinator -y And then going back into the root workspace cd ~/aerostack2_ws sudo rosdep init rosdep update rosdep install -y -r -q --from-paths src --ignore-src Then, enable the handy aerostack2 cli (only run this once) echo 'export AEROSTACK2_PATH=$HOME/aerostack2_ws/src/aerostack2' >> $HOME/.bashrc echo 'source $AEROSTACK2_PATH/as2_cli/setup_env.bash' >> $HOME/.bashrc echo 'source $HOME/aerostack2_ws/install/setup.bash' >> $HOME/.bashrc source ~/.bashrc This will enable you to build the project from any folder using as2 build Now as2 should be installed.","title":"Setup and build Aerostack2 (we use version 1.1.2)"},{"location":"simulation/installation/#setup-this-project","text":"Get this project locally mkdir -p ~/aerostack2_ws/src cd ~/aerostack2_ws/src git clone https://github.com/ucl-delta/project_gazebo_aruco.git Run the example using cd /ros2/project_gazebo_aruco ./launch_as2.bash -s -t","title":"Setup this project"},{"location":"simulation/installation/#docker","text":"Ensure Docker or Docker Desktop is installed on your machine First your will need to clone this project somewhere (doesn't need to be in a ros2 workspace) git clone https://github.com/ucl-delta/project_gazebo_aruco.git To build and/or run the container run the script This container is based on Ubuntu 22.04, ROS2 Humble and Ignition Gazebo Fortress ./docker/docker_start.bash After building for a while, this will drop you inside the docker container. The container will have live mounted this project into /ros2/project_gazebo_aruco so that any changes made to this repository outside of the container will be reflected inside. Inside the container, navigate to that repository and run the example. cd /ros2/project_gazebo_aruco ./launch_as2.bash -s -t Note that you can utilise a GPU if you install the nvidia-container-toolkit . Pass the -nvidia argument to docker_start.bash","title":"Docker"},{"location":"simulation/installation/#docker-with-vnc","text":"This will enable all of the ROS2 to run standalone with no outside network connections. It makes use of a Virtual Network Computing interface to share a the container's desktop GUI with the outside world. In this case your browser! Ensure Docker or Docker Desktop is installed on your machine First your will need to clone this project somewhere (doesn't need to be in a ros2 workspace) git clone https://github.com/ucl-delta/project_gazebo_aruco.git To build and/or run the container run the script This container is based on Ubuntu 22.04, ROS2 Humble and Ignition Gazebo Fortress ./docker/docker_vnc_start.bash After building for a while, this will say that it has started the VNC server. Note in some instances, the first time you build it it may time out. Run it again and it should finish its build. Go into a browser and navigate to https://127.0.0.1:6080 and press connect . This will drop you into an ubunut22.04 desktop environment with all the things you need! The container will have live mounted this project into /ros2/project_gazebo_aruco so that any changes made to this repository outside of the container will be reflected inside. Open up a terminal ( terminator ) and navigate to /ros2/project_gazebo_aruco to run the example. cd /ros2/project_gazebo_aruco ./launch_as2.bash -s -t Note that you can utilise a GPU if you install the nvidia-container-toolkit . Pass the -nvidia argument to docker_vnc_start.bash when running the vnc.","title":"Docker with VNC"},{"location":"simulation/installation/#environment-installation","text":"This section applies if you are trying to setup from scratch This project relies on ROS2 Humble (End-of-life 2027), Ignition Gazebo Fortress, and Ubuntu 22.04. This section goes over how to install these elements. You may want to refer to the Intro To Linux Tutorial to setup some of the dependencies.","title":"Environment Installation"},{"location":"simulation/installation/#installing-ubuntu-2204","text":"Your options are Finding a spare machine that you can wipe and install Ubuntu on Finding a machine that you would be willing to dual boot Running a virtual machine Running a container (Not supported yet as I havent made a container - also this requires gazebo which requires a GUI which is not ideal for containers apart from singularity containers) For options 1 and 2, you will need to find a USB stick that is >4Gb and using a tool such as Rufus flash the Ubuntu 22.04 ISO file onto it. Once you have a flashed USB drive, you can insert that into the spare machine. On startup make sure to mash some combination of F2, F8 or F12 to go to the BIOS boot screen and select boot from USB. This will start up tp the Ubuntu installer on the USB drive where you can select what to do. Whether that is to wipe the machine, or in the advanced menu create a new partition for Ubuntu so you can dual boot (For Windows you will also need to shrink your primary partition). For more details see a guide such as this one . You can also download the ISO file and run it in a virtual machine program such as VirtualBox . I have created and exported a virtual machine with everything already installed for you to use. See the private teams group. In virtualbox you can import an existing virtual machine. Once installed and you have the gui up, there is an option to import (orange arrow), in which you can select the existing exported virtual machine. Once you have a virtual machine setup, you can simply start it. Note: Since this project uses gazebo and is not the lightest workioad, you may want to give the VM more resources i.e. CPU, RAM and potetially video memory too. You can do this in the virtual machine settings.","title":"Installing Ubuntu 22.04"},{"location":"simulation/installation/#installing-ros2-humble","text":"For this project we will be using ROS2 Humble. Full installation instructions are here . But in short: locale # check for UTF-8 sudo apt update && sudo apt install locales sudo locale-gen en_US en_US.UTF-8 sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8 export LANG=en_US.UTF-8 locale # verify settings # Install sudo apt install software-properties-common sudo add-apt-repository universe sudo apt update && sudo apt install curl -y sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null # Update sudo apt update sudo apt upgrade # Install ROS2 sudo apt install ros-humble-desktop sudo apt install ros-dev-tools # Auto Source in bashrc to have access to ros2 tools echo 'source ~/opt/ros/huble/setup.bash' >> $HOME/.bashrc","title":"Installing ROS2 Humble"},{"location":"simulation/installation/#installing-ignition-gazebo-fortress","text":"The recommended compatible gazebo version for Ubuntu 22.04 and Humble is Fortress where installation instructions are here . But in short: sudo apt-get update sudo apt-get install lsb-release wget gnupg sudo wget https://packages.osrfoundation.org/gazebo.gpg -O /usr/share/keyrings/pkgs-osrf-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/pkgs-osrf-archive-keyring.gpg] http://packages.osrfoundation.org/gazebo/ubuntu-stable $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/gazebo-stable.list > /dev/null sudo apt-get update sudo apt-get install ignition-fortress You can check succesful installation by using the ign cli command or gz cli command With these three installed, you should be ready to run the sample gazebo aruco simulation","title":"Installing Ignition Gazebo Fortress"},{"location":"simulation/intro_ros2_uav/","text":"An Introduction to ROS2 and UAV Control \u00b6 This tutorial gives a brief overview and background on UAV Control and ROS2. By the end you should have a brief understanding of how a UAV is controlled, how drone systems treat a UAV and why and how we use ROS2 to communicate with a UAV. An Introduction to ROS2 and UAV Control A Brief Introduction to UAV Control How do you control a UAV The Autopilot MAVLink, XRCEDDS and Autopilot communication A Brief Introduction to ROS Why does ROS exist? What is ROS ROS concepts through an example ROS2 vs ROS1 Communicating between ROS2 and Drones XRCE-DDS and ROS MAVLINK and ROS with MAVROS Next Steps A Brief Introduction to UAV Control \u00b6 How do you control a UAV \u00b6 Modified from ardupilot docs A multicopter is a mechanically simple aerial vehicle whose motion is controlled by speeding or slowing multiple downward thrusting motor/propeller units. Combining different thrusts on different rotors allows the vehicle to move in free space with 6 degrees of freedom. However, manually controlling the individual thrusts of each motor in order to move the UAV is incredibly difficult, most would say its impossible even. This instability means that an on-board computer is mandatory for stable flight, as the on-board controller can perform the extreme high-rate control required to keep the drone in the air. In this \"Fly by wire\" paradigm, if the computer isn't working, you aren't flying. This dedicated on-board controller is referred to as the autopilot . This is seperate from a companion computer which is often used to direct the autopilot to achieve higher level mission goals. The autopilot combines data from small on-board MEMs gyroscopes and accelerometers (the same as those found in smart phones) to maintain an accurate estimate of its orientation and position. The quadcopter shown above is the simplest type of multicopter, with each motor/propeller spinning in the opposite direction from the two motors on either side of it (i.e. motors on opposite corners of the frame spin in the same direction). A quadcopter can control its roll and pitch rotation by speeding up two motors on one side and slowing down the other two. So for example if the quadcopter wanted to roll left it would speed up motors on the right side of the frame and slow down the two on the left. Similarly if it wants to rotate forward it speeds up the back two motors and slows down the front two. The copter can turn (aka \u201cyaw\u201d) left or right by speeding up two motors that are diagonally across from each other, and slowing down the other two. Horizontal motion is accomplished by temporarily speeding up/slowing down some motors so that the vehicle is leaning in the direction of desired travel and increasing the overall thrust of all motors so the vehicle shoots forward. Generally the more the vehicle leans, the faster it travels. Altitude is controlled by speeding up or slowing down all motors at the same time. In order to automatically map higher level motions to the thrust of the rotors, a cascading set of PID controllers is designed and provided by the autopilot. These then allow the remote control flight of the vehicle from a transmitter in your pilots hands, or via messages sent by the companion computer The Autopilot \u00b6 There is no universal controller design of converting from user inputs to motor thrust. In the same way, there are numerous other functionalities that an autopilot can cover. These can range from running control loops for gimbals, cameras and other actuation, to high level mission following and safety features. These functionalities are bundled into specific autopilot firmwares which each offer a slightly different set of features, as well as differing user interfaces each with their advantages and drawbacks. The two current most common autopilot firmware's in use in research settings are Ardupilot which offers the Arducopter firmware, and PX4 which offers Multicopter firmware. Both these firmwares are very extensive and cover numerous use cases. However, for our purposes we will only cover enabling autonomous flight through observing the mode of the autpilot. Note that other autopilots such as Betaflight , INAV and others used for other use cases. Both Ardupilot and PX4 use the concept of flight modes, where each mode operates a supports different levels or types of flight stabilisation and/or autonomous functions. Traditionally this is for pilots to change between different controller layouts for different applications. It's necessary to change to the correct mode for safe and controllable flight. The following table shows the most often used flight modes within Starling. Ardupilot Mode PX4 Mode Functionality stabilized manual Full manual control with RC sticks being sent directly to control roll, pitch, yaw and height PosHold position UAV uses onboard sensing to stay in place, RC sticks used to translate position loiter auto.hold Automatic mode where UAV stays in the same location until further instructions given. land auto.land Automatic mode which attempts to land the UAV Guided offboard Navigates to setpoints sent to it by ground control or companion computer Our controllers will all ask the autopilot to switch into guided or offboard mode in order to control from the companion computer. Often they have safety elements build in which mean that the autopilot must receive instructions at a certain rate (2Hz) otherwise the autopilot will switch to loiter or land. As mentioned before, the firmware provides a given cascading PID controller for converting high level commands to motor thrusts. As a controller developer, it is also useful to understand the differences between the Ardupilot and PX4 controllers and what real world impacts that has. In our own work, it has generally been noted that Ardupilot seems to be more suitable for outdoor flight, and PX4 for indoor flight. For this tutorial we will be developing a controller for indoor flight and so we will assume the use of PX4. MAVLink, XRCEDDS and Autopilot communication \u00b6 Once in guided or offboard mode, the autopilot expects communications using one of two protocls. The first traditional method is via the MAVLINK protocol . Traditionally this would have been used for a ground control station (GCS) to send commands to a UAV over a telemetry link. However, now it has also developed into a protocol for commanding the autopilot from an onboard companion computer over a USB or serial connection too. The MAVLink protocol is a set of preset commands which compatible firmwares understand and react to. However, it is often verbose and not-intuitive to develop applications with, as well as requiring a lot of prior knowledge about the state of the system. For example, it is neccesary to send a number of specific messages in order to receive individual data streams on vehicle status, location, global location and so on. These are often missed and cause lots of headaches for developers. Starling aims to streamline this through the use of the Robot Operating System so users no longer need to interact with MAVLink and the autopilot directly. However in recent years, autopilot developers have noticed this growing trend of autopilots communicating directly with companion computers through mavlink, and have seen its issues. Therefore a new protocol has been developed specifically for interfacing with ROS called XRCE-DDS. This new protocol is directly compatible with ROS and doesnt require any translation, serialisation or deserialisation between the two systems, greatly improving efficiency and enabling higher frequency control. A Brief Introduction to ROS \u00b6 This section is adapted from this article ROS stands for the Robot Operating System, yet it isn't an actual operating system. It's a framework designed to expedite the development time of robot platforms. To understand what ROS is, we should understand why ROS exists in the first place. Why does ROS exist? \u00b6 In general, software developers avoid hardware like the plague. It's messy, doesn't have consistent behavior, and there's no ctrl-z in sight. Most beginner programmers think you have to have a deep knowledge of electronics and even mechanics to program robots. They think that the hardware and software are so tightly coupled, you have to know both in depth to build anything useful. Software developers became software developers for a reason, so they don't have to deal with hardware. For example, let's say you have to debug a faulty sensor. You first have to take out the sensor from the enclosure, test the sensor thoroughly with a multi meter and various test cases, document its behavior, then examine the hardware -level code to ensure that there were no bugs, and so on. That's a lot of interaction with the hardware that's not fun for someone who just wants to write some cool software. It's harder to attract good programmers if the programming is coupled deeply with hardware. This is where ROS comes into play. With ROS, you can completely abstract the hardware from software, and instead interact with an API that gives access to that data. You can forget about the hardware, and focus on developing the software that makes the robot do what you want. What is ROS \u00b6 ROS is essentially a framework that sits on top of an operating system which defines how particular ROS compatible programs communicate and share data with each other. Essentially ROS defines an interface between which compatible programs can communicate and interact with each other. Over the years that ROS has existed, many people have developed thousands of ROS compatible packages which can be used in a modular fashion. ROS concepts through an example \u00b6 To make it more concrete, imagine that on your drone you have a camera. There are also two processes which require, as inputs, that camera image. Say, a machine learning program, and a position estimation program. Traditionally, you would have to manually serialise (compress) and stream the image over a port which the other two programs could read from. But if the port changes or, say, the camera changes, lots of things have to be reconfigured. However, this sort of interaction can be made streamlined in ROS. Let us consider the programs we have as ROS nodes , i.e. a program which is responsible for one single modular purpose, with particular inputs or outputs: A camera image streaming node OUT: camera image A machine vision system for recognising objects IN: camera image OUT: list of recognised objects A simultaneous localisation and mapping system. IN: camera image OUT: vehicle position These outputs of a node define ROS topics , i.e. a single stream of one type of data. Each topic has a particular name which can be referred to. In our example, some of the topics might be: /drone/camera for the camera image /drone/recognised_objects for the machine vision system /drone/slam_position for the SLAM system Then, we see that there are two avenues of communication created from these node inputs and outputs. graph LR A[Camera] -->|out| node[drone/camera] node --in--> C[Machine Vision] node --in--> D[SLAM] style node fill:#f9f,stroke:#333,stroke-width:4px Now ROS follows a publisher/subscriber model of communication. What that means is that nodes publish data to topics as outputs. But that data is only sent across the network if a different nodes also subscribes to the same topic. So in our example we end up having A camera image streaming node OUT: publishing to /drone/camera A machine vision system for recognising objects IN: subscribed to /drone/camera OUT: publishing to /drone/recognised_objects A simultaneous localisation and mapping system. IN: subscribed to /drone/camera OUT: publishing to /drone/slam_position graph LR A[Camera] -->|out| node[drone/camera] node --in--> C[Vision] C -->|out| node1[drone/recognised_objects] node --in--> D[SLAM] D -->|out| node2[drone/slam_position] style node fill:#f9f,stroke:#333,stroke-width:4px style node1 fill:#f9f,stroke:#333,stroke-width:4px style node2 fill:#f9f,stroke:#333,stroke-width:4px Finally, the data that is sent is not just anything. The data or message is a specifically templated packet of data containing things specified for that paricular use case. In our example for /drone/slam_position topic, the message might be of type geometry_msgs/msg/Point.msg which is defined like so: # This contains the position of a point in free space float64 x float64 y float64 z In other words the message that the /drone/slam_position topic publishes must have a msg.x , msg.y and msg.z field, and the subscriber will only receivea message with those fields. There are a number of messages in the standard ROS library, but many libraries also define their own - as have we in some parts of Starling. This can be summarised in this diagram from the ROS tutorials demonstrates it very nicely: The bottom half of this shows how topics get sent from a publisher to a subscriber. Interestingly, if you put two topics together, you get some notion of two way communication. This is the basis of a service which can be seen in the top of the diagram. A service is made of a Request topic and a Response topic, but functions as a single communication type to the user. Similar to messages, a service has a defined request and response types (e.g. see std_srvs/srv/SetBool.srv ). A service request will often wait until a response is received before continuing. Then if you combine two services and a topic, you can imitate a request for something which takes time. This in ROS is known as an action . For example requesting a robot to move from one location to another. You first request the move, which you get a response as to whether its started. This is followed by constant feedback along a particular topic. Then ended with a task complete service response. In this a whole set of messages are defined. Note that everything happens asyncronously and in parallel, when a node subscribes or sends a requests, it doesn't know when the response will arrive. It only knows it will (hopefully) arrive at some point. When a packet is received the subscriber can then run a method - this method is usually known as a callback , but that will be covered in a later tutorial. Finally, each node is configured by a set of parameters which are broadcast to all other nodes. Parameters are often configuration values for particular methods in a node, and can sometimes be changed on startup (or dynamically through a service), to allow the node to provide adjustable functionality. For example the value of a timeout or frequency of a loop. So in summary, the key concepts and terminology are: Nodes Topics Publishers and Subscribers Messages Services Actions Parameters ROS2 vs ROS1 \u00b6 There are 2 versions of ROS: ROS1 and ROS2. ROS1, initially created in 2007 by Willow Garage, has become huge among the open source robotics community. However over the years they realised that there are a number of important features which are missing - and adding all of these would simply break ROS1. Also the most recent ROS1 distribution (ROS Noetic) is soon to reach the end of its supported life (EOL 2025) with no more ROS1 there after! (See this article for more details!) Therefore, to future proof the system, and to ensure all users get a well rounded experience that will hopefully translate to industry experience, Starling has been implemented in ROS2. Specifically, this tutorial uses the Humble Hawksbill (AKA Humble) Long Term Support (LTS) distribution throughout. There are some interesting changes between ROS1 and ROS2, but the core elements described above remain identical. For those interested, ROS2 follows a much more decentralised paradigm, and does not require a central ROSnode as it uses the distributed DDS communication protocol for its internal communication. All nodes therefore broadcast their own topics allowing for easy decentralised discovery - perfect for multi-robot applications. Note: Main thing to be aware of is if you are debugging and searching for ROS questions on the internet, be aware that there are many existing questions for ROS1 which will no longer apply for ROS2. Communicating between ROS2 and Drones \u00b6 Coming back round to flying drones, we mentioned earlier that we want ROS to communicate with the autopilot in the most straightforward way possible. Ideally communicate in a way that provides a consistent (ROS) interface with which we can build controllers against! XRCE-DDS and ROS \u00b6 PX4 can use the uXRCE-DDS middleware to allow the internal uORB messages to be published and subscribed on a companion computer as though they were ROS 2 topics. This provides a fast and reliable integration between PX4 and ROS 2, and makes it much easier for ROS 2 applications to get vehicle information and send commands. Essentially on the host computer side (companion computer or laptop), you connect up a telemetry port into the USB and you can run the XRCE-DDS-Agent. This translates all of the available vehicle messages into useable ros2 topics which can be viewer and browsed. MAVLINK and ROS with MAVROS \u00b6 For referece, we also include a little bit about MAVROS here as internet searches may still include a number of discussions regarding MAVLINK and MAVROS. While less used now, it is still a robust method for communicating with MAVLINK enabled systems, and enables a user to tap into the existing MAVLINK ecosystem. This will sill be the primary method when interfacing with Ardupilot for example. For the autpilot, it automatically sets up a connection and translates higher level ROS commands into MAVLINK commands. For controller developers, Mavros provides a known and consistent interface through a set of topics, services and parameters to interact with. These include high level actions such as requesting the vehicle's state, local position, gps position, as well as setting setpoints for the vehicle to visit. A couple of useful topics are in the following table: Name Topic Message Type Functionality State mavros/state mavros_msgs/msg/State Get's the current state and flight mode of the vehicle Local Position mavros/local_position/pose geometry_msgs/msg/PoseStamped Get the UAVs current coordinate position after sensor fusion GPS Position mavros/global_position/global sensor_msgs/msg/NavSatFix Get the UAVs current lat,long (if enabled) Position Setpoint mavros/setpoint_position/local geometry_msgs/msg/PoseStamped Send a target coordinate and orientation for the vehicle to fly to immediately Set Flight Mode mavros/set_mode mavros_msgs/srv/SetMode A service which sets the flight mode of the autopilot Set Data Stream Rate mavros/set_stream_rate mavros_msgs/srv/StreamRate A service which starts the data stream from the autopilot and sets its rate Sometimes, you may need to send raw MAVlink back to the Autopilot to enable some non-standard functionality. This can also be done through the MAVROS node too. As we are now utilising ROS, this allows us to make the most of the full ROS ecosystem in developing UAV applications. Next Steps \u00b6 Hopefully now you have a basic understanding of what a drone is and how they are controlled, the function and purpose of an autopilot, as well as how ROS functions can be used. If you want some early hands on experience with ROS before delving further into drone work, we highly recommend the offical ros2 tutorials . Just to introduce it here, whilst we now have the ability to interface and send data and commands to the drone. We probably need a lot of the higher level functionality in order to properly fly the drone. That is where frameworks such as Aerostack2 come into play. We will discuss this in the followin tutorials.","title":"Intro to ROS2 and UAVs"},{"location":"simulation/intro_ros2_uav/#an-introduction-to-ros2-and-uav-control","text":"This tutorial gives a brief overview and background on UAV Control and ROS2. By the end you should have a brief understanding of how a UAV is controlled, how drone systems treat a UAV and why and how we use ROS2 to communicate with a UAV. An Introduction to ROS2 and UAV Control A Brief Introduction to UAV Control How do you control a UAV The Autopilot MAVLink, XRCEDDS and Autopilot communication A Brief Introduction to ROS Why does ROS exist? What is ROS ROS concepts through an example ROS2 vs ROS1 Communicating between ROS2 and Drones XRCE-DDS and ROS MAVLINK and ROS with MAVROS Next Steps","title":"An Introduction to ROS2 and UAV Control"},{"location":"simulation/intro_ros2_uav/#a-brief-introduction-to-uav-control","text":"","title":"A Brief Introduction to UAV Control"},{"location":"simulation/intro_ros2_uav/#how-do-you-control-a-uav","text":"Modified from ardupilot docs A multicopter is a mechanically simple aerial vehicle whose motion is controlled by speeding or slowing multiple downward thrusting motor/propeller units. Combining different thrusts on different rotors allows the vehicle to move in free space with 6 degrees of freedom. However, manually controlling the individual thrusts of each motor in order to move the UAV is incredibly difficult, most would say its impossible even. This instability means that an on-board computer is mandatory for stable flight, as the on-board controller can perform the extreme high-rate control required to keep the drone in the air. In this \"Fly by wire\" paradigm, if the computer isn't working, you aren't flying. This dedicated on-board controller is referred to as the autopilot . This is seperate from a companion computer which is often used to direct the autopilot to achieve higher level mission goals. The autopilot combines data from small on-board MEMs gyroscopes and accelerometers (the same as those found in smart phones) to maintain an accurate estimate of its orientation and position. The quadcopter shown above is the simplest type of multicopter, with each motor/propeller spinning in the opposite direction from the two motors on either side of it (i.e. motors on opposite corners of the frame spin in the same direction). A quadcopter can control its roll and pitch rotation by speeding up two motors on one side and slowing down the other two. So for example if the quadcopter wanted to roll left it would speed up motors on the right side of the frame and slow down the two on the left. Similarly if it wants to rotate forward it speeds up the back two motors and slows down the front two. The copter can turn (aka \u201cyaw\u201d) left or right by speeding up two motors that are diagonally across from each other, and slowing down the other two. Horizontal motion is accomplished by temporarily speeding up/slowing down some motors so that the vehicle is leaning in the direction of desired travel and increasing the overall thrust of all motors so the vehicle shoots forward. Generally the more the vehicle leans, the faster it travels. Altitude is controlled by speeding up or slowing down all motors at the same time. In order to automatically map higher level motions to the thrust of the rotors, a cascading set of PID controllers is designed and provided by the autopilot. These then allow the remote control flight of the vehicle from a transmitter in your pilots hands, or via messages sent by the companion computer","title":"How do you control a UAV"},{"location":"simulation/intro_ros2_uav/#the-autopilot","text":"There is no universal controller design of converting from user inputs to motor thrust. In the same way, there are numerous other functionalities that an autopilot can cover. These can range from running control loops for gimbals, cameras and other actuation, to high level mission following and safety features. These functionalities are bundled into specific autopilot firmwares which each offer a slightly different set of features, as well as differing user interfaces each with their advantages and drawbacks. The two current most common autopilot firmware's in use in research settings are Ardupilot which offers the Arducopter firmware, and PX4 which offers Multicopter firmware. Both these firmwares are very extensive and cover numerous use cases. However, for our purposes we will only cover enabling autonomous flight through observing the mode of the autpilot. Note that other autopilots such as Betaflight , INAV and others used for other use cases. Both Ardupilot and PX4 use the concept of flight modes, where each mode operates a supports different levels or types of flight stabilisation and/or autonomous functions. Traditionally this is for pilots to change between different controller layouts for different applications. It's necessary to change to the correct mode for safe and controllable flight. The following table shows the most often used flight modes within Starling. Ardupilot Mode PX4 Mode Functionality stabilized manual Full manual control with RC sticks being sent directly to control roll, pitch, yaw and height PosHold position UAV uses onboard sensing to stay in place, RC sticks used to translate position loiter auto.hold Automatic mode where UAV stays in the same location until further instructions given. land auto.land Automatic mode which attempts to land the UAV Guided offboard Navigates to setpoints sent to it by ground control or companion computer Our controllers will all ask the autopilot to switch into guided or offboard mode in order to control from the companion computer. Often they have safety elements build in which mean that the autopilot must receive instructions at a certain rate (2Hz) otherwise the autopilot will switch to loiter or land. As mentioned before, the firmware provides a given cascading PID controller for converting high level commands to motor thrusts. As a controller developer, it is also useful to understand the differences between the Ardupilot and PX4 controllers and what real world impacts that has. In our own work, it has generally been noted that Ardupilot seems to be more suitable for outdoor flight, and PX4 for indoor flight. For this tutorial we will be developing a controller for indoor flight and so we will assume the use of PX4.","title":"The Autopilot"},{"location":"simulation/intro_ros2_uav/#mavlink-xrcedds-and-autopilot-communication","text":"Once in guided or offboard mode, the autopilot expects communications using one of two protocls. The first traditional method is via the MAVLINK protocol . Traditionally this would have been used for a ground control station (GCS) to send commands to a UAV over a telemetry link. However, now it has also developed into a protocol for commanding the autopilot from an onboard companion computer over a USB or serial connection too. The MAVLink protocol is a set of preset commands which compatible firmwares understand and react to. However, it is often verbose and not-intuitive to develop applications with, as well as requiring a lot of prior knowledge about the state of the system. For example, it is neccesary to send a number of specific messages in order to receive individual data streams on vehicle status, location, global location and so on. These are often missed and cause lots of headaches for developers. Starling aims to streamline this through the use of the Robot Operating System so users no longer need to interact with MAVLink and the autopilot directly. However in recent years, autopilot developers have noticed this growing trend of autopilots communicating directly with companion computers through mavlink, and have seen its issues. Therefore a new protocol has been developed specifically for interfacing with ROS called XRCE-DDS. This new protocol is directly compatible with ROS and doesnt require any translation, serialisation or deserialisation between the two systems, greatly improving efficiency and enabling higher frequency control.","title":"MAVLink, XRCEDDS and Autopilot communication"},{"location":"simulation/intro_ros2_uav/#a-brief-introduction-to-ros","text":"This section is adapted from this article ROS stands for the Robot Operating System, yet it isn't an actual operating system. It's a framework designed to expedite the development time of robot platforms. To understand what ROS is, we should understand why ROS exists in the first place.","title":"A Brief Introduction to ROS"},{"location":"simulation/intro_ros2_uav/#why-does-ros-exist","text":"In general, software developers avoid hardware like the plague. It's messy, doesn't have consistent behavior, and there's no ctrl-z in sight. Most beginner programmers think you have to have a deep knowledge of electronics and even mechanics to program robots. They think that the hardware and software are so tightly coupled, you have to know both in depth to build anything useful. Software developers became software developers for a reason, so they don't have to deal with hardware. For example, let's say you have to debug a faulty sensor. You first have to take out the sensor from the enclosure, test the sensor thoroughly with a multi meter and various test cases, document its behavior, then examine the hardware -level code to ensure that there were no bugs, and so on. That's a lot of interaction with the hardware that's not fun for someone who just wants to write some cool software. It's harder to attract good programmers if the programming is coupled deeply with hardware. This is where ROS comes into play. With ROS, you can completely abstract the hardware from software, and instead interact with an API that gives access to that data. You can forget about the hardware, and focus on developing the software that makes the robot do what you want.","title":"Why does ROS exist?"},{"location":"simulation/intro_ros2_uav/#what-is-ros","text":"ROS is essentially a framework that sits on top of an operating system which defines how particular ROS compatible programs communicate and share data with each other. Essentially ROS defines an interface between which compatible programs can communicate and interact with each other. Over the years that ROS has existed, many people have developed thousands of ROS compatible packages which can be used in a modular fashion.","title":"What is ROS"},{"location":"simulation/intro_ros2_uav/#ros-concepts-through-an-example","text":"To make it more concrete, imagine that on your drone you have a camera. There are also two processes which require, as inputs, that camera image. Say, a machine learning program, and a position estimation program. Traditionally, you would have to manually serialise (compress) and stream the image over a port which the other two programs could read from. But if the port changes or, say, the camera changes, lots of things have to be reconfigured. However, this sort of interaction can be made streamlined in ROS. Let us consider the programs we have as ROS nodes , i.e. a program which is responsible for one single modular purpose, with particular inputs or outputs: A camera image streaming node OUT: camera image A machine vision system for recognising objects IN: camera image OUT: list of recognised objects A simultaneous localisation and mapping system. IN: camera image OUT: vehicle position These outputs of a node define ROS topics , i.e. a single stream of one type of data. Each topic has a particular name which can be referred to. In our example, some of the topics might be: /drone/camera for the camera image /drone/recognised_objects for the machine vision system /drone/slam_position for the SLAM system Then, we see that there are two avenues of communication created from these node inputs and outputs. graph LR A[Camera] -->|out| node[drone/camera] node --in--> C[Machine Vision] node --in--> D[SLAM] style node fill:#f9f,stroke:#333,stroke-width:4px Now ROS follows a publisher/subscriber model of communication. What that means is that nodes publish data to topics as outputs. But that data is only sent across the network if a different nodes also subscribes to the same topic. So in our example we end up having A camera image streaming node OUT: publishing to /drone/camera A machine vision system for recognising objects IN: subscribed to /drone/camera OUT: publishing to /drone/recognised_objects A simultaneous localisation and mapping system. IN: subscribed to /drone/camera OUT: publishing to /drone/slam_position graph LR A[Camera] -->|out| node[drone/camera] node --in--> C[Vision] C -->|out| node1[drone/recognised_objects] node --in--> D[SLAM] D -->|out| node2[drone/slam_position] style node fill:#f9f,stroke:#333,stroke-width:4px style node1 fill:#f9f,stroke:#333,stroke-width:4px style node2 fill:#f9f,stroke:#333,stroke-width:4px Finally, the data that is sent is not just anything. The data or message is a specifically templated packet of data containing things specified for that paricular use case. In our example for /drone/slam_position topic, the message might be of type geometry_msgs/msg/Point.msg which is defined like so: # This contains the position of a point in free space float64 x float64 y float64 z In other words the message that the /drone/slam_position topic publishes must have a msg.x , msg.y and msg.z field, and the subscriber will only receivea message with those fields. There are a number of messages in the standard ROS library, but many libraries also define their own - as have we in some parts of Starling. This can be summarised in this diagram from the ROS tutorials demonstrates it very nicely: The bottom half of this shows how topics get sent from a publisher to a subscriber. Interestingly, if you put two topics together, you get some notion of two way communication. This is the basis of a service which can be seen in the top of the diagram. A service is made of a Request topic and a Response topic, but functions as a single communication type to the user. Similar to messages, a service has a defined request and response types (e.g. see std_srvs/srv/SetBool.srv ). A service request will often wait until a response is received before continuing. Then if you combine two services and a topic, you can imitate a request for something which takes time. This in ROS is known as an action . For example requesting a robot to move from one location to another. You first request the move, which you get a response as to whether its started. This is followed by constant feedback along a particular topic. Then ended with a task complete service response. In this a whole set of messages are defined. Note that everything happens asyncronously and in parallel, when a node subscribes or sends a requests, it doesn't know when the response will arrive. It only knows it will (hopefully) arrive at some point. When a packet is received the subscriber can then run a method - this method is usually known as a callback , but that will be covered in a later tutorial. Finally, each node is configured by a set of parameters which are broadcast to all other nodes. Parameters are often configuration values for particular methods in a node, and can sometimes be changed on startup (or dynamically through a service), to allow the node to provide adjustable functionality. For example the value of a timeout or frequency of a loop. So in summary, the key concepts and terminology are: Nodes Topics Publishers and Subscribers Messages Services Actions Parameters","title":"ROS concepts through an example"},{"location":"simulation/intro_ros2_uav/#ros2-vs-ros1","text":"There are 2 versions of ROS: ROS1 and ROS2. ROS1, initially created in 2007 by Willow Garage, has become huge among the open source robotics community. However over the years they realised that there are a number of important features which are missing - and adding all of these would simply break ROS1. Also the most recent ROS1 distribution (ROS Noetic) is soon to reach the end of its supported life (EOL 2025) with no more ROS1 there after! (See this article for more details!) Therefore, to future proof the system, and to ensure all users get a well rounded experience that will hopefully translate to industry experience, Starling has been implemented in ROS2. Specifically, this tutorial uses the Humble Hawksbill (AKA Humble) Long Term Support (LTS) distribution throughout. There are some interesting changes between ROS1 and ROS2, but the core elements described above remain identical. For those interested, ROS2 follows a much more decentralised paradigm, and does not require a central ROSnode as it uses the distributed DDS communication protocol for its internal communication. All nodes therefore broadcast their own topics allowing for easy decentralised discovery - perfect for multi-robot applications. Note: Main thing to be aware of is if you are debugging and searching for ROS questions on the internet, be aware that there are many existing questions for ROS1 which will no longer apply for ROS2.","title":"ROS2 vs ROS1"},{"location":"simulation/intro_ros2_uav/#communicating-between-ros2-and-drones","text":"Coming back round to flying drones, we mentioned earlier that we want ROS to communicate with the autopilot in the most straightforward way possible. Ideally communicate in a way that provides a consistent (ROS) interface with which we can build controllers against!","title":"Communicating between ROS2 and Drones"},{"location":"simulation/intro_ros2_uav/#xrce-dds-and-ros","text":"PX4 can use the uXRCE-DDS middleware to allow the internal uORB messages to be published and subscribed on a companion computer as though they were ROS 2 topics. This provides a fast and reliable integration between PX4 and ROS 2, and makes it much easier for ROS 2 applications to get vehicle information and send commands. Essentially on the host computer side (companion computer or laptop), you connect up a telemetry port into the USB and you can run the XRCE-DDS-Agent. This translates all of the available vehicle messages into useable ros2 topics which can be viewer and browsed.","title":"XRCE-DDS and ROS"},{"location":"simulation/intro_ros2_uav/#mavlink-and-ros-with-mavros","text":"For referece, we also include a little bit about MAVROS here as internet searches may still include a number of discussions regarding MAVLINK and MAVROS. While less used now, it is still a robust method for communicating with MAVLINK enabled systems, and enables a user to tap into the existing MAVLINK ecosystem. This will sill be the primary method when interfacing with Ardupilot for example. For the autpilot, it automatically sets up a connection and translates higher level ROS commands into MAVLINK commands. For controller developers, Mavros provides a known and consistent interface through a set of topics, services and parameters to interact with. These include high level actions such as requesting the vehicle's state, local position, gps position, as well as setting setpoints for the vehicle to visit. A couple of useful topics are in the following table: Name Topic Message Type Functionality State mavros/state mavros_msgs/msg/State Get's the current state and flight mode of the vehicle Local Position mavros/local_position/pose geometry_msgs/msg/PoseStamped Get the UAVs current coordinate position after sensor fusion GPS Position mavros/global_position/global sensor_msgs/msg/NavSatFix Get the UAVs current lat,long (if enabled) Position Setpoint mavros/setpoint_position/local geometry_msgs/msg/PoseStamped Send a target coordinate and orientation for the vehicle to fly to immediately Set Flight Mode mavros/set_mode mavros_msgs/srv/SetMode A service which sets the flight mode of the autopilot Set Data Stream Rate mavros/set_stream_rate mavros_msgs/srv/StreamRate A service which starts the data stream from the autopilot and sets its rate Sometimes, you may need to send raw MAVlink back to the Autopilot to enable some non-standard functionality. This can also be done through the MAVROS node too. As we are now utilising ROS, this allows us to make the most of the full ROS ecosystem in developing UAV applications.","title":"MAVLINK and ROS with MAVROS"},{"location":"simulation/intro_ros2_uav/#next-steps","text":"Hopefully now you have a basic understanding of what a drone is and how they are controlled, the function and purpose of an autopilot, as well as how ROS functions can be used. If you want some early hands on experience with ROS before delving further into drone work, we highly recommend the offical ros2 tutorials . Just to introduce it here, whilst we now have the ability to interface and send data and commands to the drone. We probably need a lot of the higher level functionality in order to properly fly the drone. That is where frameworks such as Aerostack2 come into play. We will discuss this in the followin tutorials.","title":"Next Steps"},{"location":"simulation/intro_to_linux/","text":"A Brief Introduction to Linux \u00b6 Adapted from this digital ocean tutorial What is Linux \u00b6 Linux is a family of free and open-source operating systems based on the Linux kernel (core operating system). Operating systems based on Linux are known as Linux distributions or distros. Examples include Debian, Ubuntu, Fedora, CentOS, Gentoo, Arch Linux, and many others. The Linux kernel has been under active development since 1991, and has proven to be extremely versatile and adaptable. You can find computers that run Linux in a wide variety of contexts all over the world, from web servers to cell phones. Today, 90% of all cloud infrastructure and 74% of the world\u2019s smartphones are powered by Linux. However, newcomers to Linux may find it somewhat difficult to approach, as Linux filesystems have a different structure than those found on Windows or MacOS. Additionally, Linux-based operating systems depend heavily on working with the command line interface, while most personal computers rely on graphical interfaces. The Terminal \u00b6 The terms \u201cterminal,\u201d \u201cshell,\u201d and \u201ccommand line interface\u201d are often used interchangeably, but there are subtle differences between them: A terminal is an input and output environment that presents a text-only window running a shell. A shell is a program that exposes the computer\u2019s operating system to a user or program. In Linux systems, the shell presented in a terminal is a command line interpreter. The default shell in Ubuntu Linux is known as bash . A command line interface is a user interface (managed by a command line interpreter program) which processes commands to a computer program and outputs the results. When someone refers to one of these three terms in the context of Linux, they generally mean a terminal environment where you can run commands and see the results printed out to the terminal, such as this: There are two ways to open a terminal: Pressing the Win or Cmd key to open the program menu and typing terminal , then pressing Enter Pressing Ctrl + Alt + T This default terminal is known as the 'gnome-terminal'. Other terminals exist such as 'terminator' Becoming a Linux user requires you to be comfortable with using a terminal. Any administrative task, including file manipulation, package installation, and user management, can be accomplished through the terminal. The terminal is interactive: you specify commands to run (after the $ sign) and the terminal outputs the results of those commands. To execute any command, you type it into the prompt and press Enter . When using the Starling system, interacting with Docker and ROS2, you'll most often be doing so through a terminal shell. Although personal computers that run Linux often come with the kind of graphical desktop environment familiar to most computer users, it is often more efficient or practical to perform certain tasks through commands entered into the terminal. As of writing, a GUI (Graphical User Interface) has not been developed for Starling, and so almost all tasks have to be achieved through the terminal shell. A basic command to try out is echo , which will print things to the terminal. For example echo hello-world will print hello-world into the terminal. You can also use it to observe the value of Environment Variables which record and keep useful variables to the operation of the Operating System. For example, when you run a command in bash , bash will look for the command executable in the locations provided by the environment variable PATH . You can print the contents of this env-var using echo $PATH . The $ before the name of the variable tells bash that the following word represents an environment variable, and that it should be looked up. Navigating the file system \u00b6 Like Windows and Mac, the Linux filesystems are based on a directory tree. This means that you can create directories (which are functionally identical to folders found in other operating systems) inside other directories, and files can exist in any directory. The forward slash ( / ) is used to indicate the root directory in the filesystem hierarchy. When a user logs in to the shell, they are brought to their own user directory, stored within /home/<username> . This is referred to as the user\u2019s home directory. Often you may see the tilde ( ~ ) character when specifying a file location (e.g. ~/Documents/hello.txt = /home/<username>/Documents/hello.txt ). This is shorthand for the user's home directory and gets substituted in when used. To see what directory you are currently active in you can run the pwd command, which stands for \u201cprint working directory\u201d myuser@my-machine:~$ pwd /home/myuser To see a list of files and directories that exist in your current working directory, run the ls command: myuser@my-machine:~$ ls Desktop Documents Downloads Pictures Public Wallpapers You can get more details if you run ls -al command: myuser@my-machine:~$ ls -al drwxr-xr-x 2 myuser myuser 4096 Apr 30 2021 Desktop drwxrwxr-x 8 myuser myuser 4096 Oct 29 09:27 Documents drwxrwxr-x 8 myuser myuser 4096 Dec 10 14:41 Downloads drwxrwxr-x 8 myuser myuser 4096 May 23 10:43 Pictures drwxrwxr-x 8 myuser myuser 4096 Jan 19 2017 Public drwxrwxr-x 8 myuser myuser 4096 Oct 15 09:43 Wallpapers You can create one or more new directories within your current working directory with the mkdir command, which stands for \u201cmake directory\u201d. For example, to create two new directories named testdir1 and testdir2, you might run the first command. You can create nested directories by using the -p option: myuser@my-machine:~$ mkdir testdir1 testdir2 myuser@my-machine:~$ mkdir -p testdir3/testsubdir To navigate into one of these new directories, run the cd command (which stands for \u201cchange directory\u201d) and specify the directory\u2019s name: myuser@my-machine:~$ cd testdir1 myuser@my-machine:~/testdir1$ Note that you can navigate from anywhere to anywhere. cd only requires a valid filepath. Note also that . represents the current folder and .. represents the parent folder. Note also how is shows the current working directory in the shell as well. cd # This will bring you back to home directory cd testdir3/testsubdir # Brings you into testsubdir cd ../ # Brings you back out one level into testdir3 cd ../testdir1 # Brings you back out one level and back into testdir1 cd /home/<username>/testdir2 # Absolute reference to testdir2 cd ~/testdir2 # Absolute reference using tilde to testdir2 Working with files \u00b6 You cannot use cd to interact with files; cd stands for \u201cchange directory\u201d, and only allows you to navigate directories. You can, however, create, edit, and view the contents of files. One way to create a file is with the touch command. This creates an empty file in your current working directory. To create a new file called file.txt: touch file.txt If you decide to rename file.txt later on, you can do so with the mv command. mv stands for \u201cmove\u201d and it can move a file or directory from one place to another. By specifying the original file, file.txt, you can \u201cmove\u201d it to a new location in the current working directory, thereby renaming it. mv file.txt newfile.txt It is also possible to copy a file to a new location with the cp command. If we want to copy newfile.txt, you can make a copy of newfile.txt named newfile_copy.txt like this: cp newfile.txt newfile_copy.txt However, files are not of much use if they don\u2019t contain anything. To edit files, a file editor is necessary. There are many options for file editors, all created by professionals for daily use. Such editors include vim, emacs, nano, and pico. nano is a suitable option for beginners: it is relatively user-friendly and doesn\u2019t overload you with cryptic options or commands. nano file.txt This will open a space where you can start typing to edit the file. In nano specifically you can save your written text by pressing Ctrl + X , Y , and then Enter . This returns you to the shell with a newly saved file.txt . Now that file.txt has some text within it, you can view it using cat or less . The cat command prints the contents of a specified file to your system\u2019s output. Try running cat and pass the file.txt file you just edited as an argument: cat file.txt Using cat to view file contents can be unwieldy and difficult to read if the file is particularly long. As an alternative, you can use the less command which will allow you to paginate the output. Use less to view the contents of the file.txt file, like this: less file.txt This will also print the contents of file.txt, but one terminal page at a time beginning at the start of the file. You can use the spacebar to advance a page, or the arrow keys to go up and down one line at a time. Press Q to quit out of less . Finally, to delete the file.txt file, pass the name of the file as an argument to rm : rm file.txt rm -d directory rmidr directory rm -r directory # If the directory you are deleting is not empty NOTE : If your question has to do with a specific Linux command, the manual pages offer detailed and insightful documentation for nearly every command. To see the man page for any command, pass the command\u2019s name as an argument to the man command - man command . For instance, man rm displays the purpose of rm , how to use it, what options are available, examples of use, and more useful information. NOTE : If a command fails or is hanging or you just want to stop it, most of the time you can stop the running process by pressing Ctrl + C . This will send a Keyboard Interrupt message to the program and hopefully stop it. Installing Dependencies and Useful Programs \u00b6 Like windows and mac, individual programs can be manually downloaded (usually as a tar.gz file instead of exe ) and manually installed into your operating system (using dpkg ). However, the linux project offers a much more straight forward method through the apt (Advanced Packaging Tool) utility. apt is a free-software user interface that works with core libraries to handle the installation and removal of software on Debian operating systems like Ubuntu. (For other distributions you may come across equivalents like yum ). This is the primary method for installing software onto your system. To use apt , and more specifically apt-get which 'gets' programs for you, you must first run the update command to get the current list of all available software. Note that because sudo is used, you will most likely need to input your password. sudo will be explained below. sudo apt-get update Note that it will hang (stop responding) or fail if you are not connected to the internet. Installing Git and VSCode \u00b6 You can then install your programs using apt-get install . For Starling, you will need to use the git version control software to both download Starling and eventually build your own controllers. To install git , run the following: sudo apt-get install git We also recommend the use of Visual Studio Code as your development environment or text editor, but you are free to use whatever you want (atom, notepad++ etc etc). We heavily make use of it during development and recommend a number of extensions. VScode can be installed using the snap utility. snap is a slightly more modern successor to apt for more general programs. snap comes ready installed on your linux distrubtion. sudo snap install code --classic sudo \u00b6 Now in these commands, we have prefixed all of them with sudo . sudo these days usually stands for superuser do and allows a command to be run with the privileges of the superuser (aka the root user), if the user has been given permissions to do so. Any command which installs or modifies directories outside of the users home directory will often need superuser privileges to avoid non-superusers from changing things they shouldn't. As the above commands all seek to install programs to the system, they need superuser permissions to do so. Running without sudo will give you a permission error. Running a command with sudo will ask you for your own accounts password. Installing Docker \u00b6 For Linux systems, see the following install page . There are multiple ways of installation docker, but we recommend installing using the repository method: Update the apt repository and install requirements sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Add Docker's repository: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install Docker (and docker-compose!): sudo apt-get update sudo apt-get install docker-ce docker-ce-cli docker-compose containerd.io Test Docker installation: sudo docker run hello-world This will install Docker, accessible using sudo root privileges only. To use docker without sudo, run the following (there are security issues with this, but it doesn't matter for running Starling locally) Run the following sudo groupadd docker sudo usermod -aG docker $USER Log out and log in again to enforce changes Verify that it was successful (we will come back to this command): docker run hello-world That is Docker on Linux installed. See the original page for any further details.","title":"A Brief Introduction to Linux"},{"location":"simulation/intro_to_linux/#a-brief-introduction-to-linux","text":"Adapted from this digital ocean tutorial","title":"A Brief Introduction to Linux"},{"location":"simulation/intro_to_linux/#what-is-linux","text":"Linux is a family of free and open-source operating systems based on the Linux kernel (core operating system). Operating systems based on Linux are known as Linux distributions or distros. Examples include Debian, Ubuntu, Fedora, CentOS, Gentoo, Arch Linux, and many others. The Linux kernel has been under active development since 1991, and has proven to be extremely versatile and adaptable. You can find computers that run Linux in a wide variety of contexts all over the world, from web servers to cell phones. Today, 90% of all cloud infrastructure and 74% of the world\u2019s smartphones are powered by Linux. However, newcomers to Linux may find it somewhat difficult to approach, as Linux filesystems have a different structure than those found on Windows or MacOS. Additionally, Linux-based operating systems depend heavily on working with the command line interface, while most personal computers rely on graphical interfaces.","title":"What is Linux"},{"location":"simulation/intro_to_linux/#the-terminal","text":"The terms \u201cterminal,\u201d \u201cshell,\u201d and \u201ccommand line interface\u201d are often used interchangeably, but there are subtle differences between them: A terminal is an input and output environment that presents a text-only window running a shell. A shell is a program that exposes the computer\u2019s operating system to a user or program. In Linux systems, the shell presented in a terminal is a command line interpreter. The default shell in Ubuntu Linux is known as bash . A command line interface is a user interface (managed by a command line interpreter program) which processes commands to a computer program and outputs the results. When someone refers to one of these three terms in the context of Linux, they generally mean a terminal environment where you can run commands and see the results printed out to the terminal, such as this: There are two ways to open a terminal: Pressing the Win or Cmd key to open the program menu and typing terminal , then pressing Enter Pressing Ctrl + Alt + T This default terminal is known as the 'gnome-terminal'. Other terminals exist such as 'terminator' Becoming a Linux user requires you to be comfortable with using a terminal. Any administrative task, including file manipulation, package installation, and user management, can be accomplished through the terminal. The terminal is interactive: you specify commands to run (after the $ sign) and the terminal outputs the results of those commands. To execute any command, you type it into the prompt and press Enter . When using the Starling system, interacting with Docker and ROS2, you'll most often be doing so through a terminal shell. Although personal computers that run Linux often come with the kind of graphical desktop environment familiar to most computer users, it is often more efficient or practical to perform certain tasks through commands entered into the terminal. As of writing, a GUI (Graphical User Interface) has not been developed for Starling, and so almost all tasks have to be achieved through the terminal shell. A basic command to try out is echo , which will print things to the terminal. For example echo hello-world will print hello-world into the terminal. You can also use it to observe the value of Environment Variables which record and keep useful variables to the operation of the Operating System. For example, when you run a command in bash , bash will look for the command executable in the locations provided by the environment variable PATH . You can print the contents of this env-var using echo $PATH . The $ before the name of the variable tells bash that the following word represents an environment variable, and that it should be looked up.","title":"The Terminal"},{"location":"simulation/intro_to_linux/#navigating-the-file-system","text":"Like Windows and Mac, the Linux filesystems are based on a directory tree. This means that you can create directories (which are functionally identical to folders found in other operating systems) inside other directories, and files can exist in any directory. The forward slash ( / ) is used to indicate the root directory in the filesystem hierarchy. When a user logs in to the shell, they are brought to their own user directory, stored within /home/<username> . This is referred to as the user\u2019s home directory. Often you may see the tilde ( ~ ) character when specifying a file location (e.g. ~/Documents/hello.txt = /home/<username>/Documents/hello.txt ). This is shorthand for the user's home directory and gets substituted in when used. To see what directory you are currently active in you can run the pwd command, which stands for \u201cprint working directory\u201d myuser@my-machine:~$ pwd /home/myuser To see a list of files and directories that exist in your current working directory, run the ls command: myuser@my-machine:~$ ls Desktop Documents Downloads Pictures Public Wallpapers You can get more details if you run ls -al command: myuser@my-machine:~$ ls -al drwxr-xr-x 2 myuser myuser 4096 Apr 30 2021 Desktop drwxrwxr-x 8 myuser myuser 4096 Oct 29 09:27 Documents drwxrwxr-x 8 myuser myuser 4096 Dec 10 14:41 Downloads drwxrwxr-x 8 myuser myuser 4096 May 23 10:43 Pictures drwxrwxr-x 8 myuser myuser 4096 Jan 19 2017 Public drwxrwxr-x 8 myuser myuser 4096 Oct 15 09:43 Wallpapers You can create one or more new directories within your current working directory with the mkdir command, which stands for \u201cmake directory\u201d. For example, to create two new directories named testdir1 and testdir2, you might run the first command. You can create nested directories by using the -p option: myuser@my-machine:~$ mkdir testdir1 testdir2 myuser@my-machine:~$ mkdir -p testdir3/testsubdir To navigate into one of these new directories, run the cd command (which stands for \u201cchange directory\u201d) and specify the directory\u2019s name: myuser@my-machine:~$ cd testdir1 myuser@my-machine:~/testdir1$ Note that you can navigate from anywhere to anywhere. cd only requires a valid filepath. Note also that . represents the current folder and .. represents the parent folder. Note also how is shows the current working directory in the shell as well. cd # This will bring you back to home directory cd testdir3/testsubdir # Brings you into testsubdir cd ../ # Brings you back out one level into testdir3 cd ../testdir1 # Brings you back out one level and back into testdir1 cd /home/<username>/testdir2 # Absolute reference to testdir2 cd ~/testdir2 # Absolute reference using tilde to testdir2","title":"Navigating the file system"},{"location":"simulation/intro_to_linux/#working-with-files","text":"You cannot use cd to interact with files; cd stands for \u201cchange directory\u201d, and only allows you to navigate directories. You can, however, create, edit, and view the contents of files. One way to create a file is with the touch command. This creates an empty file in your current working directory. To create a new file called file.txt: touch file.txt If you decide to rename file.txt later on, you can do so with the mv command. mv stands for \u201cmove\u201d and it can move a file or directory from one place to another. By specifying the original file, file.txt, you can \u201cmove\u201d it to a new location in the current working directory, thereby renaming it. mv file.txt newfile.txt It is also possible to copy a file to a new location with the cp command. If we want to copy newfile.txt, you can make a copy of newfile.txt named newfile_copy.txt like this: cp newfile.txt newfile_copy.txt However, files are not of much use if they don\u2019t contain anything. To edit files, a file editor is necessary. There are many options for file editors, all created by professionals for daily use. Such editors include vim, emacs, nano, and pico. nano is a suitable option for beginners: it is relatively user-friendly and doesn\u2019t overload you with cryptic options or commands. nano file.txt This will open a space where you can start typing to edit the file. In nano specifically you can save your written text by pressing Ctrl + X , Y , and then Enter . This returns you to the shell with a newly saved file.txt . Now that file.txt has some text within it, you can view it using cat or less . The cat command prints the contents of a specified file to your system\u2019s output. Try running cat and pass the file.txt file you just edited as an argument: cat file.txt Using cat to view file contents can be unwieldy and difficult to read if the file is particularly long. As an alternative, you can use the less command which will allow you to paginate the output. Use less to view the contents of the file.txt file, like this: less file.txt This will also print the contents of file.txt, but one terminal page at a time beginning at the start of the file. You can use the spacebar to advance a page, or the arrow keys to go up and down one line at a time. Press Q to quit out of less . Finally, to delete the file.txt file, pass the name of the file as an argument to rm : rm file.txt rm -d directory rmidr directory rm -r directory # If the directory you are deleting is not empty NOTE : If your question has to do with a specific Linux command, the manual pages offer detailed and insightful documentation for nearly every command. To see the man page for any command, pass the command\u2019s name as an argument to the man command - man command . For instance, man rm displays the purpose of rm , how to use it, what options are available, examples of use, and more useful information. NOTE : If a command fails or is hanging or you just want to stop it, most of the time you can stop the running process by pressing Ctrl + C . This will send a Keyboard Interrupt message to the program and hopefully stop it.","title":"Working with files"},{"location":"simulation/intro_to_linux/#installing-dependencies-and-useful-programs","text":"Like windows and mac, individual programs can be manually downloaded (usually as a tar.gz file instead of exe ) and manually installed into your operating system (using dpkg ). However, the linux project offers a much more straight forward method through the apt (Advanced Packaging Tool) utility. apt is a free-software user interface that works with core libraries to handle the installation and removal of software on Debian operating systems like Ubuntu. (For other distributions you may come across equivalents like yum ). This is the primary method for installing software onto your system. To use apt , and more specifically apt-get which 'gets' programs for you, you must first run the update command to get the current list of all available software. Note that because sudo is used, you will most likely need to input your password. sudo will be explained below. sudo apt-get update Note that it will hang (stop responding) or fail if you are not connected to the internet.","title":"Installing Dependencies and Useful Programs"},{"location":"simulation/intro_to_linux/#installing-git-and-vscode","text":"You can then install your programs using apt-get install . For Starling, you will need to use the git version control software to both download Starling and eventually build your own controllers. To install git , run the following: sudo apt-get install git We also recommend the use of Visual Studio Code as your development environment or text editor, but you are free to use whatever you want (atom, notepad++ etc etc). We heavily make use of it during development and recommend a number of extensions. VScode can be installed using the snap utility. snap is a slightly more modern successor to apt for more general programs. snap comes ready installed on your linux distrubtion. sudo snap install code --classic","title":"Installing Git and VSCode"},{"location":"simulation/intro_to_linux/#sudo","text":"Now in these commands, we have prefixed all of them with sudo . sudo these days usually stands for superuser do and allows a command to be run with the privileges of the superuser (aka the root user), if the user has been given permissions to do so. Any command which installs or modifies directories outside of the users home directory will often need superuser privileges to avoid non-superusers from changing things they shouldn't. As the above commands all seek to install programs to the system, they need superuser permissions to do so. Running without sudo will give you a permission error. Running a command with sudo will ask you for your own accounts password.","title":"sudo"},{"location":"simulation/intro_to_linux/#installing-docker","text":"For Linux systems, see the following install page . There are multiple ways of installation docker, but we recommend installing using the repository method: Update the apt repository and install requirements sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Add Docker's repository: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install Docker (and docker-compose!): sudo apt-get update sudo apt-get install docker-ce docker-ce-cli docker-compose containerd.io Test Docker installation: sudo docker run hello-world This will install Docker, accessible using sudo root privileges only. To use docker without sudo, run the following (there are security issues with this, but it doesn't matter for running Starling locally) Run the following sudo groupadd docker sudo usermod -aG docker $USER Log out and log in again to enforce changes Verify that it was successful (we will come back to this command): docker run hello-world That is Docker on Linux installed. See the original page for any further details.","title":"Installing Docker"}]}